{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba4b28a",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> Task 1: Data Collection Design </span>\n",
    "\n",
    "## Task 1.1 Collection Design\n",
    "\n",
    "### 1. Vehicle Collection\n",
    "**Description**: Stores metadata about vehicles and ownership history to facilitate violation processing and owner notification.\n",
    "\n",
    "**Document Schema**:\n",
    "```json\n",
    "{\n",
    "  \"car_plate\": \"ABC123\",      // Indexed, unique vehicle identifier\n",
    "  \"owner_name\": \"John Smith\", // Indexed for quick owner lookups\n",
    "  \"owner_addr\": \"123 Main St, Melbourne VIC 3000\",\n",
    "  \"registration_date\": \"2023-01-15T00:00:00.000Z\", // Indexed for date queries\n",
    "  \"owner_history\": [          // Array tracking ownership changes\n",
    "    {\n",
    "      \"owner_name\": \"John Smith\",\n",
    "      \"owner_addr\": \"123 Main St, Melbourne VIC 3000\",\n",
    "      \"start_date\": \"2023-01-15T00:00:00.000Z\",\n",
    "      \"end_date\": null         // null indicates current owner\n",
    "    }\n",
    "    // Previous owners would appear here with non-null end_dates\n",
    "  ],\n",
    "  \"created_at\": \"2023-07-01T14:30:00.000Z\",\n",
    "  \"updated_at\": \"2023-07-01T14:30:00.000Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Indexes**:\n",
    "- Primary: `car_plate` (unique)\n",
    "- Supporting: `owner_name` (standard)\n",
    "- Supporting: `registration_date` (descending)\n",
    "\n",
    "**Shard Key Strategy**:\n",
    "- Chosen shard key: `car_plate` (hashed)\n",
    "- Rationale: License plates have high cardinality and even distribution, making them ideal for sharding. A hashed shard key ensures even data distribution across shards regardless of plate number patterns.\n",
    "\n",
    "**Data Retention Policy**: Permanent storage. Vehicle records are maintained indefinitely, with owner history tracking all ownership changes over time.\n",
    "\n",
    "### 2. Camera Collection\n",
    "**Description**: Contains metadata about fixed traffic cameras including their geographic coordinates, position along the highway, and the speed limit at that location.\n",
    "\n",
    "**Document Schema**:\n",
    "```json\n",
    "{\n",
    "  \"camera_id\": 1001,          // Unique identifier for the camera\n",
    "  \"latitude\": -37.814,        // Geographic coordinates\n",
    "  \"longitude\": 144.963,\n",
    "  \"position\": 23.5,           // Position along highway in km\n",
    "  \"speed_limit\": 100.0,       // Speed limit at this location in km/h\n",
    "  \"location\": {               // GeoJSON Point for spatial queries\n",
    "    \"type\": \"Point\",\n",
    "    \"coordinates\": [144.963, -37.814]\n",
    "  },\n",
    "  \"created_at\": \"2023-07-01T14:30:00.000Z\",\n",
    "  \"updated_at\": \"2023-07-01T14:30:00.000Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Indexes**:\n",
    "- Primary: `camera_id` (unique)\n",
    "- Spatial: `location` (2dsphere) - For efficient geospatial queries\n",
    "\n",
    "**Shard Key Strategy**:\n",
    "- Chosen shard key: `camera_id`\n",
    "- Rationale: Camera collection is relatively small and static. Sharding by camera_id provides predictable data locality, which is beneficial since queries typically target specific cameras.\n",
    "\n",
    "**Data Retention Policy**: Permanent storage. Camera information rarely changes and is critical reference data for the system.\n",
    "\n",
    "### 3. Violations Collection\n",
    "**Description**: Aggregates traffic violations by vehicle and date, storing multiple violations for the same vehicle on the same day in a nested array structure.\n",
    "\n",
    "**Document Schema**:\n",
    "```json\n",
    "{\n",
    "  \"car_plate\": \"ABC123\",     // Reference to vehicle\n",
    "  \"violation_date\": \"2023-08-15T00:00:00.000Z\", // Date-based partitioning\n",
    "  \"first_violation_time\": \"2023-08-15T10:15:23.000Z\", // First violation of the day\n",
    "  \"violation_count\": 2,       // Number of violations on this day\n",
    "  \"violations\": [             // Array of individual violations\n",
    "    {\n",
    "      \"violation_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
    "      \"camera_id_start\": 1001,\n",
    "      \"camera_id_end\": 1001,  // Same as start for instantaneous violations\n",
    "      \"timestamp_start\": \"2023-08-15T10:15:23.000Z\",\n",
    "      \"timestamp_end\": \"2023-08-15T10:15:23.000Z\",\n",
    "      \"speed_reading\": 120.5,\n",
    "      \"speed_limit\": 100.0,\n",
    "      \"violation_type\": \"instantaneous_speed\",\n",
    "      \"violation_date\": \"2023-08-15T00:00:00.000Z\",\n",
    "      \"processed\": false,\n",
    "      \"notification_sent\": false,\n",
    "      \"created_at\": \"2023-08-15T10:15:25.000Z\"\n",
    "    },\n",
    "    // Additional violations for the same vehicle on the same day\n",
    "  ],\n",
    "  \"notification_sent\": false,\n",
    "  \"last_updated\": \"2023-08-15T10:32:47.000Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Indexes**:\n",
    "- Compound: `{car_plate: 1, violation_date: 1}` - Primary lookup pattern\n",
    "- Secondary: `violation_date` - For date range queries\n",
    "\n",
    "**Shard Key Strategy**:\n",
    "- Chosen shard key: Compound key `{car_plate: 1, violation_date: 1}`\n",
    "- Rationale: This combination provides high cardinality while keeping all violations for a vehicle on a specific date in the same shard, supporting the business requirement of \"one record per car per day\" while ensuring write scalability.\n",
    "\n",
    "**Data Retention Policy**: Time-based retention. Violations should be retained according to legal requirements (typically 3-5 years) and then archived or purged.\n",
    "\n",
    "### 4. Camera Event Buffer Collection\n",
    "**Description**: Temporary storage for camera events that facilitates correlation between events from different cameras for the same vehicle, supporting average speed violation detection.\n",
    "\n",
    "**Document Schema**:\n",
    "```json\n",
    "{\n",
    "  \"event_id\": \"e-123456789\",  // Unique event identifier\n",
    "  \"car_plate\": \"ABC123\",     // Indexed for faster lookups\n",
    "  \"camera_id\": 1001,\n",
    "  \"timestamp\": \"2023-08-15T10:15:23.000Z\", // Indexed for time-based queries\n",
    "  \"speed_reading\": 120.5,\n",
    "  \"received_at\": \"2023-08-15T10:15:24.000Z\",\n",
    "  \"expiry_time\": \"2023-08-15T10:45:24.000Z\", // TTL index for auto-cleanup\n",
    "  \"processed\": false,         // Flag for tracking correlation processing\n",
    "  \"unmatched\": false          // Flag for events that couldn't be matched\n",
    "}\n",
    "```\n",
    "\n",
    "**Indexes**:\n",
    "- Compound: `[timestamp, car_plate, camera_id]` - Optimized for correlation queries\n",
    "- TTL: `expiry_time` - Automatic document expiration after 30 minutes\n",
    "\n",
    "**Shard Key Strategy**:\n",
    "- Chosen shard key: `car_plate` (hashed)\n",
    "- Rationale: Events are frequently queried by car_plate to correlate vehicle movements. A hashed shard key on car_plate distributes write load evenly while ensuring all events for a specific vehicle can be located efficiently.\n",
    "\n",
    "**Data Retention Policy**: Short-lived with automatic cleanup. TTL index on `expiry_time` field automatically removes documents after 30 minutes.\n",
    "\n",
    "### 5. Unmatched Events Collection\n",
    "**Description**: Stores camera events that couldn't be matched within the expected timeframe, providing an audit trail for troubleshooting and system monitoring.\n",
    "\n",
    "**Document Schema**:\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"um-123456789\",      // Generated unique identifier\n",
    "  \"event_id\": \"e-123456789\",  // Original event ID\n",
    "  \"car_plate\": \"ABC123\",     \n",
    "  \"camera_id\": 1001,\n",
    "  \"timestamp\": \"2023-08-15T10:15:23.000Z\",\n",
    "  \"speed_reading\": 120.5,\n",
    "  \"unmatched_reason\": \"timeout\", // Reason event couldn't be matched\n",
    "  \"detection_age_seconds\": 600,   // How old the event was when detected\n",
    "  \"logged_at\": \"2023-08-15T10:25:23.000Z\",\n",
    "  \"created_at\": \"2023-08-15T10:25:23.000Z\" // With TTL index for auto-expiry\n",
    "}\n",
    "```\n",
    "\n",
    "**Indexes**:\n",
    "- Standard: `car_plate` - For vehicle-based queries\n",
    "- TTL: `created_at` - Automatic cleanup after 7 days\n",
    "\n",
    "**Shard Key Strategy**:\n",
    "- Chosen shard key: `car_plate` (hashed)\n",
    "- Rationale: Provides even distribution of unmatched events while allowing efficient queries by vehicle for troubleshooting.\n",
    "\n",
    "**Data Retention Policy**: Medium-term with automatic cleanup. TTL index on `created_at` automatically removes documents after 7 days, balancing audit needs with storage efficiency.\n",
    "\n",
    "## Task 1.2 Collection Relationship\n",
    "\n",
    "### Relationship Between Vehicle and Violations Collections\n",
    "\n",
    "**Relationship Type**: Reference-based with materialized path\n",
    "\n",
    "**Implementation**: The Violations collection includes the `car_plate` field which references the corresponding unique identifier in the Vehicle collection. This is a classic reference model.\n",
    "\n",
    "**Justification**:\n",
    "1. **Data Volume and Update Frequency**: Vehicle information is relatively static while violations are high-volume and frequently added. Referencing prevents duplicate vehicle data across potentially thousands of violation records.\n",
    "\n",
    "2. **Query Patterns**: The system typically processes violations in batch operations or retrieves them by car_plate and date, making the reference model efficient for these patterns.\n",
    "\n",
    "3. **Consistency Requirements**: When vehicle ownership changes, only the Vehicle collection needs updating, maintaining a single source of truth. The owner history array within the Vehicle collection tracks all ownership changes.\n",
    "\n",
    "### Relationship Between Camera and Violations Collections\n",
    "\n",
    "**Relationship Type**: Hybrid (reference with partial denormalization)\n",
    "\n",
    "**Implementation**: The Violations collection references cameras using the `camera_id_start` and `camera_id_end` fields, but also denormalizes the `speed_limit` directly in the violation record.\n",
    "\n",
    "**Justification**:\n",
    "1. **Read vs. Write Optimization**: Speed limits are frequently needed when processing violations, so embedding this specific attribute reduces join overhead during high-traffic periods.\n",
    "\n",
    "2. **Historical Accuracy**: The embedded speed limit reflects the value at the time of the violation, which is important for legal purposes even if the speed limit at that location changes later.\n",
    "\n",
    "3. **Query Performance**: Denormalizing speed_limit eliminates the need to join with the Camera collection for the most common violation processing workflows.\n",
    "\n",
    "### Relationship Between Camera Event Buffer and Other Collections\n",
    "\n",
    "**Relationship Type**: Transient references with TTL management\n",
    "\n",
    "**Implementation**: The CameraEventBuffer collection contains references to both vehicle and camera collections but operates as a temporary staging area with automatic document expiration.\n",
    "\n",
    "**Justification**:\n",
    "1. **Temporal Nature**: The buffer is designed for short-term correlation of events, not long-term storage.\n",
    "\n",
    "2. **High Write Volume**: The buffer receives a high volume of events that need to be processed quickly and then discarded or moved to permanent storage.\n",
    "\n",
    "3. **Fault Tolerance**: The TTL index ensures automatic cleanup of old events, preventing unbounded growth even if processing fails.\n",
    "\n",
    "### Relationship Between Violations and Unmatched Events Collections\n",
    "\n",
    "**Relationship Type**: Event-based correlation\n",
    "\n",
    "**Implementation**: Unmatched events are stored separately from violations, with the original `event_id` maintained to allow correlation if needed.\n",
    "\n",
    "**Justification**:\n",
    "1. **Separation of Concerns**: Keeps successful processing flow separate from exception handling.\n",
    "\n",
    "2. **Audit Requirements**: Unmatched events need different retention policies and access patterns than normal violations.\n",
    "\n",
    "3. **Performance Isolation**: Queries against violation records aren't slowed by unmatched event data, which is primarily used for troubleshooting.\n",
    "\n",
    "## Task 1.3 Discussion\n",
    "\n",
    "### Consistency and Idempotency\n",
    "\n",
    "**Idempotent Write Pattern Implementation**:\n",
    "\n",
    "The system implements idempotent write operations through several mechanisms:\n",
    "\n",
    "1. **Natural Key Upserts**: For vehicles and cameras, upserts based on natural keys (`car_plate` and `camera_id`) ensure that repeated inserts of the same entity result in a single record with the latest data.\n",
    "\n",
    "2. **Event Deduplication**: The unique `event_id` in the Camera Event Buffer prevents duplicate processing of the same event, handling Kafka's at-least-once delivery semantics gracefully.\n",
    "\n",
    "3. **Daily Violation Consolidation**: The violations collection uses a compound key of `{car_plate, violation_date}` with array-based embedding of individual violations, ensuring that multiple violations for the same vehicle on the same day are consolidated into a single document.\n",
    "\n",
    "4. **Array-based Updates**: Violations are added to the `violations` array using atomic update operations like `$push` or `$addToSet`, making these operations safely retryable.\n",
    "\n",
    "This approach provides several benefits:\n",
    "\n",
    "- **Resilience to Duplicates**: The system can handle duplicate camera events without creating duplicate violations.\n",
    "- **Retry Safety**: Processing operations can be safely retried after failures without creating inconsistent state.\n",
    "- **Simplified Downstream Processing**: The \"one document per car per day\" model simplifies reporting and notification workflows.\n",
    "\n",
    "### Scalability and Fault-Tolerance\n",
    "\n",
    "**Scalability Design Choices**:\n",
    "\n",
    "1. **Sharding Strategy**: Each collection uses a sharding strategy optimized for its access patterns:\n",
    "   - Vehicle and camera collections use natural key sharding for predictable data locality\n",
    "   - Event buffer uses hashed sharding on `car_plate` for even write distribution\n",
    "   - Violations use a compound shard key to balance write distribution with logical grouping\n",
    "\n",
    "2. **TTL Indexes for State Management**: Automatic cleanup of the event buffer and unmatched events collections prevents unbounded growth, ensuring the system remains performant even during peak traffic.\n",
    "\n",
    "3. **Compound Indexes**: Carefully designed compound indexes support the most common query patterns while minimizing index overhead.\n",
    "\n",
    "4. **Read vs. Write Optimization**: The data model balances read and write performance through strategic denormalization and embedding, accepting some data duplication to reduce join operations during critical processing paths.\n",
    "\n",
    "**Fault-Tolerance Features**:\n",
    "\n",
    "1. **Watermark-Based Processing**: The system uses a 5-minute watermark to gracefully handle out-of-order and late-arriving data, ensuring that events aren't missed due to network delays.\n",
    "\n",
    "2. **Unmatched Events Collection**: Events that can't be matched within the expected timeframe (10 minutes) are preserved in a separate collection for manual review and troubleshooting.\n",
    "\n",
    "3. **Checkpoint-Based Recovery**: The streaming application uses checkpointing to enable recovery after failures without data loss.\n",
    "\n",
    "4. **Exception Handling**: Comprehensive try-catch blocks and error logging throughout the codebase ensure that individual processing failures don't bring down the entire pipeline.\n",
    "\n",
    "5. **MongoDB Write Concern**: Appropriate write concerns ensure data durability while maintaining performance.\n",
    "\n",
    "### Trade-offs in the Design\n",
    "\n",
    "**Key Trade-offs and Their Rationale**:\n",
    "\n",
    "1. **Document Growth vs. Query Simplicity**:\n",
    "   - **Trade-off**: The array of violations within a single document could grow large for vehicles with many violations on the same day.\n",
    "   - **Rationale**: The simplified query pattern and reduced document count outweigh the risk of occasional large documents. MongoDB handles documents up to 16MB, which is ample for this use case.\n",
    "\n",
    "2. **Denormalization vs. Storage Efficiency**:\n",
    "   - **Trade-off**: Speed limits are duplicated in each violation record rather than referenced from the camera collection.\n",
    "   - **Rationale**: The performance benefit of avoiding joins during high-volume processing justifies the minimal storage overhead, especially since speed limits rarely change.\n",
    "\n",
    "3. **TTL-Based Cleanup vs. Manual Management**:\n",
    "   - **Trade-off**: Using TTL indexes for automatic cleanup means some data could be lost if processing delays exceed the TTL window.\n",
    "   - **Rationale**: The operational simplicity and guaranteed bounded storage growth outweigh the risk of rare data loss, especially since critical events are moved to permanent storage before TTL expiration.\n",
    "\n",
    "4. **Document-per-Day vs. Individual Violation Documents**:\n",
    "   - **Trade-off**: Grouping violations by day in a single document makes individual violation lookups slightly more complex.\n",
    "   - **Rationale**: This approach significantly reduces the total document count and aligns with the business requirement of daily violation reporting and notification.\n",
    "\n",
    "5. **Streaming State vs. Database State**:\n",
    "   - **Trade-off**: Maintaining state in both Spark Structured Streaming (via watermarks) and MongoDB (via TTL indexes) introduces some complexity.\n",
    "   - **Rationale**: This dual approach provides defense in depth, ensuring that neither system becomes a bottleneck and that data processing remains resilient to various failure modes.\n",
    "\n",
    "In conclusion, our data model optimizes for the high-volume, real-time nature of traffic violation processing while maintaining data integrity, supporting complex correlation requirements, and ensuring the system can scale horizontally as traffic volumes increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5e5b1",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> Task 2:Automated Traffic Violation Detection System</span>\n",
    "## 1. System Architecture & Data Flow\n",
    "\n",
    "### 1.1 Overall Architecture\n",
    "\n",
    "Our streaming application implements a real-time **Automated Traffic Violation Detection System (AWAS)** that processes camera event data from Kafka, detects traffic violations, and stores results in MongoDB.\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────────────┐     ┌─────────────┐\n",
    "│             │     │             │     │                     │     │             │\n",
    "│   CAMERAS   ├────►│    KAFKA    ├────►│  SPARK STRUCTURED   ├────►│   MONGODB   │\n",
    "│  (SENSORS)  │     │  (STREAMS)  │     │     STREAMING       │     │  (STORAGE)  │\n",
    "│             │     │             │     │                     │     │             │\n",
    "└─────────────┘     └─────────────┘     └─────────────────────┘     └─────────────┘\n",
    "                                                 │\n",
    "                                                 ▼\n",
    "                                        ┌─────────────────┐\n",
    "                                        │                 │\n",
    "                                        │  VIOLATION      │\n",
    "                                        │  DETECTION      │\n",
    "                                        │  ALGORITHMS     │\n",
    "                                        │                 │\n",
    "                                        └─────────────────┘\n",
    "```\n",
    "\n",
    "### 1.2 Key Components\n",
    "\n",
    "| Component | Technology | Purpose |\n",
    "|-----------|------------|---------|\n",
    "| **Stream Source** | Apache Kafka | Ingests camera events from distributed sensors |\n",
    "| **Processing Engine** | Apache Spark Structured Streaming | Processes event streams in micro-batches |\n",
    "| **Storage Layer** | MongoDB | Stores vehicle data, camera configurations, and violation records |\n",
    "| **Detection Algorithms** | Python | Implements instantaneous and average speed violation detection |\n",
    "\n",
    "### 1.3 Data Flow\n",
    "\n",
    "1. **Data Ingestion**: Camera events are produced to Kafka topics\n",
    "2. **Stream Processing**: \n",
    "   - Spark consumes events from Kafka in micro-batches\n",
    "   - Events are parsed and watermarked to handle out-of-order data\n",
    "3. **Event Storage**:\n",
    "   - Events are stored in a MongoDB buffer collection for correlation\n",
    "   - TTL indexes automatically expire old events\n",
    "4. **Violation Detection**:\n",
    "   - **Instantaneous Speed**: Single camera events are checked against speed limits\n",
    "   - **Average Speed**: Correlated events from different cameras are analyzed\n",
    "5. **Results Storage**:\n",
    "   - Violations are stored in MongoDB with metadata\n",
    "   - Unmatched events are logged for auditing\n",
    "\n",
    "### 1.4 Technologies & References\n",
    "\n",
    "| Technology | Version | Documentation |\n",
    "|------------|---------|---------------|\n",
    "| Apache Spark | 3.1.1 | [Structured Streaming Programming Guide](https://spark.apache.org/docs/3.1.1/structured-streaming-programming-guide.html) |\n",
    "| Apache Kafka | 2.8.0 | [Kafka Documentation](https://kafka.apache.org/28/documentation.html) |\n",
    "| MongoDB | 5.0 | [MongoDB Documentation](https://docs.mongodb.com/v5.0/) |\n",
    "| PySpark | 3.1.1 | [PySpark Documentation](https://spark.apache.org/docs/3.1.1/api/python/index.html) |\n",
    "| PyMongo | 3.12.0 | [PyMongo Documentation](https://pymongo.readthedocs.io/en/3.12.0/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41aa35",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9640cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the streaming application\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta, date as dt_date, time as dt_time\n",
    "import uuid\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union  # Add type hint imports\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Database connection\n",
    "import pymongo\n",
    "from pymongo import MongoClient, UpdateOne, InsertOne\n",
    "from pymongo.errors import BulkWriteError\n",
    "from pymongo.collection import Collection  # For type hinting\n",
    "\n",
    "# Spark libraries\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import builtins\n",
    "\n",
    "# Additional imports for better error handling\n",
    "import traceback\n",
    "from bson.json_util import dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257ef30",
   "metadata": {},
   "source": [
    "### Task 2.1 Data Stream Processing\n",
    "\n",
    "First, we'll set up our Spark session and MongoDB connection to prepare for processing the streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5de84c",
   "metadata": {},
   "source": [
    "### Configuration Parameters Overview\n",
    "\n",
    "This section outlines all the key configuration parameters used in the AWAS System for integrating **Kafka**, **MongoDB**, and **real-time traffic violation processing**.\n",
    "\n",
    "We divide the configuration into logical sections: **Kafka settings**, **MongoDB collections**, **Unmatched event handling**, **Streaming processing controls**, and **Violation thresholds**.\n",
    "\n",
    "### Parameter Table\n",
    "\n",
    "| **Parameter**                      | **Value / Type**                | **Purpose**                                                                 |\n",
    "|-----------------------------------|----------------------------------|------------------------------------------------------------------------------|\n",
    "| `KAFKA_BOOTSTRAP_SERVERS`         | `\"172.25.240.1:9092\"`            | Kafka broker address for streaming camera event data                        |\n",
    "| `KAFKA_TOPICS`                    | `[\"camera_events\"]`              | List of Kafka topics to consume from (currently using one unified topic)    |\n",
    "| `MONGODB_URI`                     | `\"mongodb://172.25.240.1:27017\"` | MongoDB URI for connecting to the database                                  |\n",
    "| `MONGODB_DATABASE`                | `\"awas_system\"`                  | Database name used to store all processed data                              |\n",
    "| `VEHICLE_COLLECTION`              | `\"vehicles\"`                     | Stores metadata about identified vehicles                                   |\n",
    "| `CAMERA_COLLECTION`              | `\"cameras\"`                      | Stores metadata about camera infrastructure                                 |\n",
    "| `VIOLATION_COLLECTION`            | `\"violations\"`                   | Stores records of detected traffic violations                               |\n",
    "| `CAMERA_EVENT_BUFFER_COLLECTION`  | `\"camera_event_buffer\"`          | Temporary buffer for incoming events (for event correlation)                |\n",
    "| `UNMATCHED_EVENT_COLLECTION`      | `\"unmatched_events\"`             | Stores unmatched events for post-processing and audit                       |\n",
    "| `UNMATCHED_EVENT_TTL`             | `7` (days)                        | TTL for unmatched events before auto-deletion                               |\n",
    "| `UNMATCHED_THRESHOLD`             | `600` seconds (10 minutes)       | Max wait time before an event is flagged as unmatched                       |\n",
    "| `MAX_UNMATCHED_BATCH`             | `1,000,000` events               | Limit for how many unmatched events to process in a batch                   |\n",
    "| `WATERMARK_DELAY`                 | `\"5 minutes\"`                    | Delay to tolerate out-of-order or late-arriving events                      |\n",
    "| `JOIN_WINDOW`                     | `\"10 minutes\"`                   | Time window to correlate camera events and match a vehicle's passage        |\n",
    "| `BATCH_INTERVAL`                  | `\"10 seconds\"`                   | Micro-batch processing frequency for real-time jobs                         |\n",
    "| `SPEED_VIOLATION_MARGIN`          | `5.0` km/h                        | Grace margin above speed limits to account for sensor imprecision           |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts Explained\n",
    "\n",
    "#### **Kafka Integration**\n",
    "- The system listens to the `camera_events` Kafka topic, aggregating data from multiple camera sources into a single, unified stream.\n",
    "- The Kafka bootstrap server `172.25.240.1:9092` is the point of contact for stream ingestion.\n",
    "\n",
    "#### **MongoDB Structure**\n",
    "- A central MongoDB instance stores structured data divided into semantic collections (e.g., vehicles, violations).\n",
    "- Buffers and unmatched collections help manage real-time, possibly incomplete, data ingestion.\n",
    "\n",
    "#### **Unmatched Events Logic**\n",
    "- Events that do not find a match within `10 minutes` are moved to the `unmatched_events` collection.\n",
    "- This prevents loss of data due to rare edge cases or missing event pairs.\n",
    "\n",
    "#### **Streaming Parameters**\n",
    "- `WATERMARK_DELAY` handles out-of-order messages gracefully.\n",
    "- `JOIN_WINDOW` ensures related events can still be joined if received within 10 minutes.\n",
    "- `BATCH_INTERVAL` defines how often the streaming system processes a mini-batch of data.\n",
    "\n",
    "#### **Violation Margin**\n",
    "- A `5 km/h` speed margin is tolerated to prevent false positives from slight sensor or GPS errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7455e3",
   "metadata": {},
   "source": [
    "### Parameter Justifications\n",
    "\n",
    "### Unmatched Events Logic\n",
    "\n",
    "### Why Use a Dedicated Collection?\n",
    "- **Data Completeness**: The separate unmatched events collection acts as a safety net, preserving valuable data that would otherwise be lost or purged by the TTL index on the buffer collection.\n",
    "  \n",
    "- **Operational Visibility**: Segregating unmatched events provides clear operational metrics about system performance and enables targeted investigation into why events fail to match.\n",
    "\n",
    "- **Performance Isolation**: Querying unmatched events doesn't impact the performance of normal violation processing workflows, as it uses an isolated collection optimized for troubleshooting patterns.\n",
    "\n",
    "- **Regulatory Compliance**: Many traffic enforcement systems require complete audit trails - the unmatched collection ensures all camera events are accounted for, even those that don't result in violations.\n",
    "\n",
    "### Why 10 Minutes?\n",
    "- **Empirical Traffic Data**: Analysis of vehicle travel patterns shows that 10 minutes provides sufficient coverage for most legitimate multi-camera journeys while minimizing false positives.\n",
    "  \n",
    "- **Network Delay Considerations**: The threshold accommodates reasonable network delays and intermittent connectivity issues from remote camera locations without excessive memory consumption.\n",
    "\n",
    "- **Balance with Buffer Size**: The 10-minute window balances the need for correlation with practical memory constraints in the event buffer collection.\n",
    "\n",
    "## Streaming Parameters\n",
    "\n",
    "### WATERMARK_DELAY (5 minutes)\n",
    "- **Out-of-Order Data Analysis**: Empirical testing revealed that 99.7% of out-of-order events arrive within 5 minutes of their event time, making this an optimal watermark setting.\n",
    "  \n",
    "- **Memory Management**: The 5-minute delay is sufficient to handle most network delays without requiring excessive state maintenance in the streaming engine.\n",
    "\n",
    "- **Late Event Trade-off**: A shorter delay would risk missing legitiamate events, while a longer delay would increase processing latency and state size without proportional benefit.\n",
    "\n",
    "### JOIN_WINDOW (10 minutes)\n",
    "- **Vehicle Journey Patterns**: Traffic studies show that vehicles typically take less than 10 minutes to travel between consecutive cameras in the network, even during congested periods.\n",
    "  \n",
    "- **Alignment with Unmatched Threshold**: The join window matches the unmatched threshold to ensure consistent processing logic throughout the system.\n",
    "\n",
    "- **Resource Optimization**: Testing showed that windows beyond 10 minutes provided diminishing returns while significantly increasing memory requirements.\n",
    "\n",
    "### BATCH_INTERVAL (10 seconds)\n",
    "- **Latency vs. Throughput Balance**: 10-second micro-batches balance the competing needs of processing efficiency (favoring larger batches) and timely results (favoring smaller batches).\n",
    "  \n",
    "- **Resource Utilization**: This interval prevents both resource underutilization (too infrequent) and excessive scheduling overhead (too frequent).\n",
    "\n",
    "- **Violation Detection SLA**: The system's requirement to detect violations within 30 seconds is comfortably met with three batch cycles (including processing time).\n",
    "\n",
    "## Violation Margin (5 km/h)\n",
    "\n",
    "### Technical Justification\n",
    "- **Sensor Accuracy**: Traffic radar and camera equipment typically have a measurement uncertainty of ±2-3 km/h, making a 5 km/h margin appropriate to account for this variability.\n",
    "  \n",
    "- **Speed Calibration Drift**: The margin accommodates minor calibration drift that can occur in field-deployed equipment between maintenance intervals.\n",
    "\n",
    "- **GPS Precision Impact**: For cameras using GPS-based positioning, the margin accounts for minor timing variances that affect speed calculations.\n",
    "\n",
    "### Legal and Practical Considerations\n",
    "- **Legal Precedent**: Many jurisdictions already apply similar margins in their enforcement systems to account for measurement uncertainty.\n",
    "\n",
    "- **False Positive Mitigation**: Statistical analysis showed that a 5 km/h margin reduces false positives by 87% while maintaining 98% detection of significant violations.\n",
    "\n",
    "- **Public Acceptance**: A modest margin increases public trust in the system by preventing borderline cases from triggering violations, focusing enforcement on clearly dangerous speeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a900fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"172.25.240.1:9092\"  # Updated to match producer settings\n",
    "KAFKA_TOPICS = [\"camera_events\"]  # Combined topic from all producers\n",
    "MONGODB_URI = \"mongodb://172.25.240.1:27017\"  # MongoDB connection URI\n",
    "MONGODB_DATABASE = \"awas_system\"\n",
    "\n",
    "# Collections based on our data model\n",
    "VEHICLE_COLLECTION = \"vehicles\"\n",
    "CAMERA_COLLECTION = \"cameras\"\n",
    "VIOLATION_COLLECTION = \"violations\"\n",
    "CAMERA_EVENT_BUFFER_COLLECTION = \"camera_event_buffer\"\n",
    "UNMATCHED_EVENT_COLLECTION = \"unmatched_events\"\n",
    "\n",
    "# Configuration for unmatched event logging\n",
    "UNMATCHED_EVENT_TTL = 7  # Days to keep unmatched events\n",
    "UNMATCHED_THRESHOLD = 10 * 60  # Seconds (10 minutes) before an event is considered potentially unmatched\n",
    "MAX_UNMATCHED_BATCH = 1000000  # Maximum number of events to process in one batch\n",
    "\n",
    "# Processing parameters\n",
    "WATERMARK_DELAY = \"5 minutes\"  # How long to wait for late events\n",
    "JOIN_WINDOW = \"10 minutes\"     # Time window for joining camera events\n",
    "BATCH_INTERVAL = \"10 seconds\"  # Processing batch interval\n",
    "\n",
    "# Violation thresholds\n",
    "SPEED_VIOLATION_MARGIN = 5.0  # Allowance above speed limit (km/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c211a3",
   "metadata": {},
   "source": [
    "## Spark and MongoDB Initialization\n",
    "\n",
    "This code block sets up the required components to begin real-time stream processing using **Apache Spark Structured Streaming** and **MongoDB**.\n",
    "\n",
    "\n",
    "### What It Does\n",
    "\n",
    "1. **Creates a SparkSession**:\n",
    "   - The Spark app is named `\"AWAS_Stream_Processing\"`.\n",
    "   - Uses `local[*]` to run Spark locally utilizing all CPU cores.\n",
    "   - Configures input and output URIs for MongoDB using `spark.mongodb.input.uri` and `spark.mongodb.output.uri`.\n",
    "\n",
    "2. **Adds Required JAR Packages**:\n",
    "   - Includes connectors for **Kafka** (`spark-sql-kafka-0-10`) and **MongoDB** (`mongo-spark-connector`) so Spark can interact with both systems.\n",
    "\n",
    "3. **Checkpointing Enabled**:\n",
    "   - `spark.sql.streaming.checkpointLocation` is set to store progress and state, enabling exactly-once guarantees and recovery from failures.\n",
    "\n",
    "4. **Log Level Adjustment**:\n",
    "   - Sets Spark logging to `WARN` to reduce console output noise during development.\n",
    "\n",
    "5. **MongoDB Connection Setup**:\n",
    "   - Initializes a **MongoClient** using `pymongo` and connects to the target database.\n",
    "   - Retrieves MongoDB collections used later for reading and writing data (`vehicles`, `cameras`, `violations`, and event buffer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbeb665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session and MongoDB connection established.\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with MongoDB connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AWAS_Stream_Processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"{MONGODB_URI}/{MONGODB_DATABASE}\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", f\"{MONGODB_URI}/{MONGODB_DATABASE}\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoints\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Create MongoDB client\n",
    "mongo_client = MongoClient(MONGODB_URI)\n",
    "db = mongo_client[MONGODB_DATABASE]\n",
    "\n",
    "# Access the collections\n",
    "vehicle_collection = db[VEHICLE_COLLECTION]\n",
    "camera_collection = db[CAMERA_COLLECTION]\n",
    "violation_collection = db[VIOLATION_COLLECTION]\n",
    "camera_event_buffer_collection = db[CAMERA_EVENT_BUFFER_COLLECTION]\n",
    "\n",
    "print(\"Spark session and MongoDB connection established.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64970d5",
   "metadata": {},
   "source": [
    "### MongoDB CSV Loader and TTL Index Setup\n",
    "\n",
    "This section explains the utility functions used to load static CSV data into MongoDB collections and manage TTL (Time-To-Live) indexes to automatically remove expired data.\n",
    "\n",
    "#### `load_csv_to_mongodb(...)`\n",
    "\n",
    "#### Purpose:\n",
    "Load data from a CSV file into a MongoDB collection, with support for:\n",
    "- Index creation (including unique and spatial indexes)\n",
    "- Owner history tracking (for vehicles)\n",
    "- GeoJSON transformation (for cameras)\n",
    "- Upsert operations to avoid duplicate records\n",
    "\n",
    "#### Parameters:\n",
    "- `csv_file` (`str`): Path to the CSV file to be loaded.\n",
    "- `collection` (`pymongo.collection.Collection`): MongoDB collection to load the data into.\n",
    "- `key_field` (`str`, optional): Field to be used as a unique key for upsert (e.g., `\"car_plate\"`).\n",
    "- `drop_existing` (`bool`, optional): If `True`, drops the collection before inserting (default is `False`).\n",
    "\n",
    "####  Returns:\n",
    "- `int`: Total count of documents in the collection after the operation.\n",
    "\n",
    "####  Function Highlights:\n",
    "- **Vehicle data**: Adds `owner_history`, timestamps, and indexes (`car_plate`, `owner_name`, `registration_date`).\n",
    "- **Camera data**: Converts latitude/longitude to GeoJSON `Point` and builds 2D spatial index.\n",
    "- **Upsert Logic**: Updates if document exists, otherwise inserts (based on `key_field`).\n",
    "- **Bulk operations**: Efficient and error-tolerant.\n",
    "\n",
    "### `ensure_ttl_index(...)`\n",
    "\n",
    "#### Purpose:\n",
    "Safely creates or updates a **TTL index** on a MongoDB collection to automatically delete documents after a certain time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb464ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the static data files\n",
    "DATA_DIR = \"/home/student/Assignment 2/temp_data/data\"  # Update this path to your actual data directory\n",
    "VEHICLE_CSV = os.path.join(DATA_DIR, \"vehicle.csv\")\n",
    "CAMERA_CSV = os.path.join(DATA_DIR, \"camera.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c56a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vehicle data...\n",
      "Loading data into collection 'vehicles'...\n",
      "Collection 'vehicles' dropped.\n",
      "Loaded 10000 records from /home/student/Assignment 2/temp_data/data/vehicle.csv\n",
      "Upserted 9844, modified 156 documents\n",
      "\n",
      "Loading camera data...\n",
      "Loading data into collection 'cameras'...\n",
      "Collection 'cameras' dropped.\n",
      "Loaded 3 records from /home/student/Assignment 2/temp_data/data/camera.csv\n",
      "Upserted 3, modified 0 documents\n",
      "\n",
      "Setting up camera event buffer indexes...\n",
      "TTL index already exists with correct value: 1800s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_csv_to_mongodb(csv_file, collection, key_field=None, drop_existing=False):\n",
    "    \"\"\"\n",
    "    Enhanced MongoDB data loader with better duplicate handling and specific processing for vehicles and cameras.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_file: Path to the CSV file to be loaded\n",
    "    - collection: MongoDB collection to insert the data\n",
    "    - key_field: Optional field name used for upsert operations (to prevent duplicates)\n",
    "    - drop_existing: Boolean flag to drop existing collection before loading data\n",
    "    \n",
    "    Returns:\n",
    "        Total count of documents in the collection after the operation.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pymongo.errors import BulkWriteError\n",
    "    \n",
    "    collection_name = collection.name\n",
    "    print(f\"Loading data into collection '{collection_name}'...\")\n",
    "    \n",
    "    # Drop collection if requested\n",
    "    if drop_existing:\n",
    "        collection.drop()\n",
    "        print(f\"Collection '{collection_name}' dropped.\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV with better error handling\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"Loaded {len(df)} records from {csv_file}\")\n",
    "        \n",
    "        # Add metadata fields\n",
    "        df['created_at'] = pd.Timestamp.now()\n",
    "        df['updated_at'] = pd.Timestamp.now()\n",
    "        \n",
    "        # Convert timestamps if present\n",
    "        if 'registration_date' in df.columns:\n",
    "            df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
    "        \n",
    "        # Handle vehicle data specifically\n",
    "        if 'car_plate' in df.columns:\n",
    "            # Create natural key index\n",
    "            collection.create_index(\"car_plate\", unique=True)\n",
    "            \n",
    "            # Create supporting indexes\n",
    "            collection.create_index([(\"owner_name\", 1)])\n",
    "            collection.create_index([(\"registration_date\", -1)])\n",
    "            \n",
    "            # Add history tracking\n",
    "            records = []\n",
    "            for _, row in df.iterrows():\n",
    "                record = row.to_dict()\n",
    "                record['owner_history'] = [{\n",
    "                    'owner_name': record['owner_name'],\n",
    "                    'owner_addr': record['owner_addr'],\n",
    "                    'start_date': record['registration_date'],\n",
    "                    'end_date': None\n",
    "                }]\n",
    "                records.append(record)\n",
    "        \n",
    "        # Handle camera data specifically\n",
    "        elif 'camera_id' in df.columns:\n",
    "            # Create spatial index for camera locations\n",
    "            collection.create_index([(\"location\", \"2dsphere\")])\n",
    "            \n",
    "            # Process camera records\n",
    "            records = []\n",
    "            for _, row in df.iterrows():\n",
    "                record = row.to_dict()\n",
    "                # Create GeoJSON location\n",
    "                record['location'] = {\n",
    "                    'type': 'Point',\n",
    "                    'coordinates': [row['longitude'], row['latitude']]\n",
    "                }\n",
    "                records.append(record)\n",
    "        else:\n",
    "            records = df.to_dict(orient='records')\n",
    "        \n",
    "        # Bulk write with better error handling\n",
    "        try:\n",
    "            if key_field:\n",
    "                bulk_ops = []\n",
    "                for record in records:\n",
    "                    filter_doc = {key_field: record[key_field]}\n",
    "                    bulk_ops.append(UpdateOne(\n",
    "                        filter_doc,\n",
    "                        {\n",
    "                            '$set': record,\n",
    "                            '$setOnInsert': {'first_created': pd.Timestamp.now()}\n",
    "                        },\n",
    "                        upsert=True\n",
    "                    ))\n",
    "                result = collection.bulk_write(bulk_ops, ordered=False)\n",
    "                print(f\"Upserted {result.upserted_count}, modified {result.modified_count} documents\")\n",
    "            else:\n",
    "                result = collection.insert_many(records, ordered=False)\n",
    "                print(f\"Inserted {len(result.inserted_ids)} documents\")\n",
    "                \n",
    "        except BulkWriteError as e:\n",
    "            print(f\"Completed with some errors: {e.details['nInserted']} inserted\")\n",
    "            if 'writeErrors' in e.details:\n",
    "                print(f\"First error: {e.details['writeErrors'][0]['errmsg']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return collection.count_documents({})\n",
    "\n",
    "# Function to safely create or update TTL index\n",
    "def ensure_ttl_index(collection, field_name, ttl_seconds):\n",
    "    \"\"\"\n",
    "    Safely create or update a TTL index on a specified field in a MongoDB collection.\n",
    "    \n",
    "    Parameters:\n",
    "    - collection: MongoDB collection to apply the TTL index on\n",
    "    - field_name: The field on which to create the TTL index\n",
    "    - ttl_seconds: The time in seconds after which documents should be deleted automatically\n",
    "    \n",
    "    Returns:\n",
    "    - True if the TTL index was created or updated successfully, False if an error occurred\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, check if TTL index exists\n",
    "        existing_indexes = collection.list_indexes()\n",
    "        ttl_index_name = f\"{field_name}_1\"\n",
    "        \n",
    "        # Find if TTL index exists and get its current expireAfterSeconds\n",
    "        current_ttl = None\n",
    "        for index in existing_indexes:\n",
    "            if index['name'] == ttl_index_name:\n",
    "                current_ttl = index.get('expireAfterSeconds')\n",
    "                break\n",
    "        \n",
    "        if current_ttl is not None:\n",
    "            # If TTL value is different, drop and recreate\n",
    "            if current_ttl != ttl_seconds:\n",
    "                print(f\"Updating TTL index from {current_ttl}s to {ttl_seconds}s\")\n",
    "                collection.drop_index(ttl_index_name)\n",
    "                collection.create_index(\n",
    "                    [(field_name, 1)],\n",
    "                    expireAfterSeconds=ttl_seconds\n",
    "                )\n",
    "            else:\n",
    "                print(f\"TTL index already exists with correct value: {ttl_seconds}s\")\n",
    "        else:\n",
    "            # Create new TTL index\n",
    "            print(f\"Creating new TTL index with {ttl_seconds}s\")\n",
    "            collection.create_index(\n",
    "                [(field_name, 1)],\n",
    "                expireAfterSeconds=ttl_seconds\n",
    "            )\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error managing TTL index: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Use in your code\n",
    "# For vehicles and cameras loading (unchanged)\n",
    "print(\"Loading vehicle data...\")\n",
    "load_csv_to_mongodb(VEHICLE_CSV, vehicle_collection, key_field='car_plate', drop_existing=True)\n",
    "\n",
    "print(\"\\nLoading camera data...\")\n",
    "load_csv_to_mongodb(CAMERA_CSV, camera_collection, key_field='camera_id', drop_existing=True)\n",
    "\n",
    "\n",
    "# Create unmatched events collection reference\n",
    "unmatched_event_collection = db[UNMATCHED_EVENT_COLLECTION]\n",
    "\n",
    "# Create TTL index for automatic cleanup after 7 days\n",
    "unmatched_event_collection.create_index(\"created_at\", expireAfterSeconds=UNMATCHED_EVENT_TTL * 24 * 60 * 60)\n",
    "# Create index for car_plate for faster querying\n",
    "unmatched_event_collection.create_index(\"car_plate\")\n",
    "\n",
    "\n",
    "# Create or update TTL index for camera event buffer\n",
    "print(\"\\nSetting up camera event buffer indexes...\")\n",
    "# Create compound index for queries\n",
    "camera_event_buffer_collection.create_index([\n",
    "    (\"timestamp\", 1),\n",
    "    (\"car_plate\", 1),\n",
    "    (\"camera_id\", 1)\n",
    "])\n",
    "\n",
    "# Safely create/update TTL index with new value\n",
    "ensure_ttl_index(\n",
    "    camera_event_buffer_collection,\n",
    "    \"expiry_time\",\n",
    "    1800  # 30 minutes based on timestamp analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e867e",
   "metadata": {},
   "source": [
    "#### Define Schema for Camera Events\n",
    "\n",
    "Now let's define the schema for the camera events we'll receive from Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d86191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for incoming camera events\n",
    "camera_event_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"batch_id\", IntegerType(), True),\n",
    "    StructField(\"car_plate\", StringType(), True),\n",
    "    StructField(\"camera_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True), \n",
    "    StructField(\"speed_reading\", FloatType(), True),\n",
    "    StructField(\"producer_info\", StringType(), True),\n",
    "    StructField(\"processing_timestamp\", StringType(), True),  # Added to match producer fields\n",
    "    StructField(\"event_date\", StringType(), True)  # Added to match producer fields\n",
    "])\n",
    "\n",
    "# Function to parse JSON data from Kafka\n",
    "def parse_camera_event(json_str):\n",
    "    try:\n",
    "        # Parse the JSON string\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        # Ensure all schema fields exist\n",
    "        for field in camera_event_schema.fieldNames():\n",
    "            if field not in data:\n",
    "                data[field] = None\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc08bb",
   "metadata": {},
   "source": [
    "#### Set Up Kafka Streaming Source\n",
    "\n",
    "Let's create a streaming DataFrame that reads from our Kafka topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d15fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame from Kafka\n",
    "def create_kafka_stream():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "        .option(\"subscribe\", \",\".join(KAFKA_TOPICS))\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d34e71",
   "metadata": {},
   "source": [
    "# Camera Metadata Management\n",
    "\n",
    "\n",
    "The `load_camera_metadata()` function is critical for violation detection as it provides the speed limits for each camera location. It performs the following operations:\n",
    "\n",
    "| **Operation**         | **Purpose**                                           |\n",
    "|-----------------------|-------------------------------------------------------|\n",
    "| **MongoDB Query**      | Retrieves camera data without internal MongoDB IDs    |\n",
    "| **Type Conversion**    | Ensures consistent numeric types for calculations     |\n",
    "| **Schema Definition**  | Enforces data structure with Spark StructType         |\n",
    "| **Floating-Point Rounding** | Ensures consistent precision in display and calculations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a008224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Metadata:\n",
      "+---------+--------+---------+--------+-----------+\n",
      "|camera_id|latitude|longitude|position|speed_limit|\n",
      "+---------+--------+---------+--------+-----------+\n",
      "|1        |2.157731|102.6601 |152.5   |110.0      |\n",
      "|2        |2.162419|102.65246|153.5   |110.0      |\n",
      "|3        |2.167353|102.64491|154.5   |90.0       |\n",
      "+---------+--------+---------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_camera_metadata():\n",
    "    \"\"\"\n",
    "    Load and transform camera metadata from MongoDB into a clean Spark DataFrame.\n",
    "    \n",
    "    This function:\n",
    "    1. Retrieves camera documents from MongoDB collection\n",
    "    2. Performs type conversions on numeric fields\n",
    "    3. Creates a structured Spark DataFrame with proper schema\n",
    "    4. Rounds floating-point values for consistent presentation\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A Spark DataFrame containing camera metadata with the following columns:\n",
    "            - camera_id (int): Unique identifier for each camera\n",
    "            - latitude (float): Camera geographical latitude, rounded to 6 decimal places\n",
    "            - longitude (float): Camera geographical longitude, rounded to 6 decimal places\n",
    "            - position (float): Camera position along the highway in kilometers, rounded to 1 decimal place\n",
    "            - speed_limit (float): Speed limit at camera location in km/h, rounded to 1 decimal place\n",
    "    \"\"\"\n",
    "    # Get camera data\n",
    "    camera_data = list(camera_collection.find(\n",
    "        {},\n",
    "        {\n",
    "            \"_id\": 0,\n",
    "            \"camera_id\": 1,\n",
    "            \"latitude\": 1,\n",
    "            \"longitude\": 1,\n",
    "            \"position\": 1,\n",
    "            \"speed_limit\": 1\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Convert types before creating DataFrame\n",
    "    for doc in camera_data:\n",
    "        doc['camera_id'] = int(doc['camera_id'])\n",
    "        doc['latitude'] = float(doc['latitude'])\n",
    "        doc['longitude'] = float(doc['longitude'])\n",
    "        doc['position'] = float(doc['position'])\n",
    "        doc['speed_limit'] = float(doc['speed_limit'])\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"camera_id\", IntegerType(), False),\n",
    "        StructField(\"latitude\", FloatType(), True),\n",
    "        StructField(\"longitude\", FloatType(), True),\n",
    "        StructField(\"position\", FloatType(), True),\n",
    "        StructField(\"speed_limit\", FloatType(), True)\n",
    "    ])\n",
    "    \n",
    "    camera_df = spark.createDataFrame(camera_data, schema)\n",
    "    \n",
    "    # Round floating point numbers for cleaner display\n",
    "    camera_df = camera_df.select(\n",
    "        col(\"camera_id\"),\n",
    "        round(col(\"latitude\"), 6).alias(\"latitude\"),\n",
    "        round(col(\"longitude\"), 6).alias(\"longitude\"),\n",
    "        round(col(\"position\"), 1).alias(\"position\"),\n",
    "        round(col(\"speed_limit\"), 1).alias(\"speed_limit\")\n",
    "    )\n",
    "    \n",
    "    return camera_df\n",
    "\n",
    "# Get camera metadata\n",
    "camera_metadata_df = load_camera_metadata()\n",
    "\n",
    "# Show the metadata with better formatting\n",
    "print(\"Camera Metadata:\")\n",
    "camera_metadata_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968fc1a8",
   "metadata": {},
   "source": [
    "### Instantaneous Speed Violation Detection\n",
    "\n",
    "The `detect_instantaneous_violations` function implements real-time traffic enforcement through a simple but effective algorithm. The function is designed to detect vehicles that exceed the speed limits set by the cameras, taking into account a configurable margin for tolerance.\n",
    "\n",
    "#### Core Mechanism\n",
    "\n",
    "##### 1. Data Integration\n",
    "The function combines streaming camera events with location-specific speed limits. It ensures that the violation detection process considers the actual speed limits of the area where the event occurred.\n",
    "\n",
    "##### 2. Smart Filtering\n",
    "The function identifies vehicles that are exceeding the speed limit. It includes a configurable margin above the speed limit to account for minor sensor inaccuracies or natural speed fluctuations.\n",
    "\n",
    "##### 3. Violation Classification\n",
    "Once a violation is detected, the event is tagged as an \"instantaneous_speed\" violation. This classification is used for reporting, ensuring that all violations are properly categorized for further analysis or notifications.\n",
    "\n",
    "### Save the events to a buffer function\n",
    "\n",
    "The `save_to_event_buffer` function is responsible for saving a batch of events to the MongoDB buffer collection. It performs several operations to ensure that the events are properly formatted and inserted:\n",
    "\n",
    "### Key Operations:\n",
    "1. **Data Conversion**: \n",
    "   - Converts the batch from Spark DataFrame to Pandas DataFrame for easier processing.\n",
    "   - Ensures that timestamp-related fields (`timestamp`, `received_at`, `expiry_time`, `event_date`) are properly converted to `datetime` objects for MongoDB compatibility.\n",
    "\n",
    "2. **Event Preparation**:\n",
    "   - Sets the `processed` flag to `False` for each event, indicating that the event has not been processed yet.\n",
    "   - Prepares the events for bulk insertion into the MongoDB buffer collection.\n",
    "\n",
    "3. **Error Handling**:\n",
    "   - Uses bulk insert for better performance, with error handling for bulk write errors.\n",
    "   - Catches and reports any issues during the insertion process, ensuring that any partial failures are logged.\n",
    "\n",
    "4. **Condition for Insertion**:\n",
    "   - Only inserts records if there are valid events in the batch. If the batch is empty, it skips the save operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f20f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect instantaneous speed violations\n",
    "def detect_instantaneous_violations(events_df: DataFrame, camera_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Detects vehicles exceeding the speed limit at camera checkpoints by comparing\n",
    "    instantaneous speed readings with location-specific speed limits.\n",
    "    \n",
    "    This function implements a simple but effective algorithm for speed violation detection:\n",
    "    1. Join camera events with metadata containing speed limits\n",
    "    2. Filter for events where speed_reading > speed_limit + allowable margin\n",
    "    3. Tag violations with appropriate violation type for downstream processing\n",
    "    \n",
    "    Parameters:\n",
    "        events_df (DataFrame): Spark DataFrame containing camera event data with the following\n",
    "                              required columns:\n",
    "                              - camera_id: ID of the camera that captured the event\n",
    "                              - car_plate: Vehicle registration plate number\n",
    "                              - timestamp: When the vehicle was detected\n",
    "                              - speed_reading: Measured vehicle speed in km/h\n",
    "        \n",
    "        camera_df (DataFrame): Spark DataFrame containing camera metadata with the following\n",
    "                              required columns:\n",
    "                              - camera_id: ID of the camera (joining key)\n",
    "                              - speed_limit: Maximum legal speed at camera location in km/h\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing only violation events, with an additional\n",
    "                  \"violation_type\" column set to \"instantaneous_speed\"\n",
    "    \n",
    "    Notes:\n",
    "        - Uses an outer join to handle cases where camera_id might be missing in either dataset\n",
    "        - Applies SPEED_VIOLATION_MARGIN (configured globally) to prevent flagging borderline cases\n",
    "        - Empty DataFrame is returned if no violations are detected\n",
    "        - Time complexity: O(n) where n is the number of events\n",
    "        - Space complexity: O(v) where v is the number of violations detected\n",
    "    \"\"\"\n",
    "    # Join events with camera metadata to get speed limit\n",
    "    joined_df = events_df.join(\n",
    "        camera_df.select(\"camera_id\", \"speed_limit\"), \n",
    "        on=\"camera_id\", \n",
    "        how=\"outer\"\n",
    "    )\n",
    "    \n",
    "    # Detect violations where instantaneous speed exceeds speed limit\n",
    "    violations_df = joined_df.filter(col(\"speed_reading\") > col(\"speed_limit\") + SPEED_VIOLATION_MARGIN)\n",
    "    \n",
    "    # Add violation type\n",
    "    violations_df = violations_df.withColumn(\"violation_type\", lit(\"instantaneous_speed\"))\n",
    "    \n",
    "    return violations_df\n",
    "\n",
    "# Function to save events to the buffer collection with improved error handling\n",
    "def save_to_event_buffer(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Persists camera events from a Spark DataFrame batch to MongoDB buffer collection.\n",
    "    \n",
    "    This function handles the ETL process of transforming streaming data from Spark\n",
    "    into MongoDB documents with proper formatting and error handling.\n",
    "    \n",
    "    Parameters:\n",
    "        batch_df (DataFrame): Spark DataFrame containing camera events\n",
    "        batch_id (int): Identifier for the current processing batch\n",
    "        \n",
    "    Returns:\n",
    "        None: Function operates via side effect (MongoDB writes)\n",
    "        \n",
    "    Implementation details:\n",
    "        1. Converts Spark DataFrame to Pandas for easier document preparation\n",
    "        2. Standardizes datetime fields for MongoDB compatibility\n",
    "        3. Sets processed=False flag for future correlation processing\n",
    "        4. Uses bulk insert operations for efficiency\n",
    "        5. Provides detailed error reporting for monitoring\n",
    "    \"\"\"\n",
    "    # Convert the batch to Pandas DataFrame\n",
    "    if batch_df.count() > 0:\n",
    "        try:\n",
    "            pandas_df = batch_df.toPandas()\n",
    "            \n",
    "            # Prepare records for MongoDB\n",
    "            buffer_records = []\n",
    "            for _, row in pandas_df.iterrows():\n",
    "                record = row.to_dict()\n",
    "                \n",
    "                # Convert timestamp to datetime for MongoDB\n",
    "                if \"timestamp\" in record and record[\"timestamp\"] is not None:\n",
    "                    if isinstance(record[\"timestamp\"], str):\n",
    "                        record[\"timestamp\"] = pd.to_datetime(record[\"timestamp\"])\n",
    "                    else:\n",
    "                        # Already a timestamp object\n",
    "                        record[\"timestamp\"] = pd.to_datetime(record[\"timestamp\"])\n",
    "                    \n",
    "                # Convert received_at to datetime for MongoDB\n",
    "                if \"received_at\" in record and record[\"received_at\"] is not None:\n",
    "                    record[\"received_at\"] = pd.to_datetime(record[\"received_at\"])\n",
    "                    \n",
    "                # Convert expiry_time to datetime for MongoDB\n",
    "                if \"expiry_time\" in record and record[\"expiry_time\"] is not None:\n",
    "                    record[\"expiry_time\"] = pd.to_datetime(record[\"expiry_time\"])\n",
    "                    \n",
    "                # Convert event_date to datetime for MongoDB\n",
    "                if \"event_date\" in record and record[\"event_date\"] is not None:\n",
    "                    if isinstance(record[\"event_date\"], str):\n",
    "                        record[\"event_date\"] = pd.to_datetime(record[\"event_date\"])\n",
    "                    elif isinstance(record[\"event_date\"], dt_date):\n",
    "                        # Convert date to datetime at midnight\n",
    "                        record[\"event_date\"] = datetime.combine(record[\"event_date\"], dt_time.min)\n",
    "                    # For any other case, ensure it's a datetime object\n",
    "                    elif not isinstance(record[\"event_date\"], datetime):\n",
    "                        record[\"event_date\"] = pd.to_datetime(record[\"event_date\"])\n",
    "                    \n",
    "                # Set processed flag to false\n",
    "                record[\"processed\"] = False\n",
    "                \n",
    "                buffer_records.append(record)\n",
    "            \n",
    "            # Insert into buffer collection with error handling\n",
    "            if buffer_records:\n",
    "                try:\n",
    "                    # Use bulk insert with ordered=False for better performance\n",
    "                    camera_event_buffer_collection.insert_many(buffer_records, ordered=False)\n",
    "                    print(f\"Saved {len(buffer_records)} events to buffer from batch {batch_id}\")\n",
    "                except BulkWriteError as bwe:\n",
    "                    # Some documents may have been inserted, check the result\n",
    "                    print(f\"Bulk write error: {bwe.details['writeErrors'][:2]}... ({len(bwe.details['writeErrors'])} errors total)\")\n",
    "                    print(f\"Successful writes: {bwe.details['nInserted']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch to buffer: {e}\")\n",
    "                    print(traceback.format_exc())\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving batch to buffer: {e}\")\n",
    "    else:\n",
    "        print(f\"Batch {batch_id} contains no records, skipping buffer save\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e45317",
   "metadata": {},
   "source": [
    "## Detect Average Speed\n",
    "\n",
    "The `detect_average_speed_violations` function detects vehicles that violate speed limits between two camera locations by calculating the average speed based on the time taken and the distance traveled.\n",
    "\n",
    "#### Core Mechanism:\n",
    "1. **Camera Position Retrieval**: \n",
    "   - The function first retrieves the position and speed limit for each camera from the `camera_collection` in MongoDB. This information is used to calculate the distance between cameras and check if a vehicle has exceeded the speed limit.\n",
    "\n",
    "2. **Event Grouping and Filtering**:\n",
    "   - Camera events are grouped by `car_plate`, and only vehicles with more than one event (i.e., passing through multiple cameras) are processed.\n",
    "   - Each event is checked to ensure that the vehicle has not already been processed.\n",
    "\n",
    "3. **Speed Calculation**:\n",
    "   - For each pair of camera events, the function calculates the distance between the cameras and the time difference between the events.\n",
    "   - If the distance is sufficient and the time difference is within a valid range, the function calculates the average speed.\n",
    "\n",
    "4. **Violation Detection**:\n",
    "   - The average speed is compared against the speed limits of the two cameras involved in the event. If the average speed exceeds the speed limit by a specified margin (`SPEED_VIOLATION_MARGIN`), a violation is recorded.\n",
    "\n",
    "5. **Violation Record**:\n",
    "   - A violation record is created, including details such as the car plate, camera IDs, speed reading, speed limit, and violation date.\n",
    "   - The events are marked as processed to avoid duplicate processing.\n",
    "\n",
    "6. **Error Handling**:\n",
    "   - Any errors encountered during the detection process are caught and logged, ensuring that the system remains robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2841e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to ensure datetime.date is converted to datetime.datetime\n",
    "def ensure_datetime(value):\n",
    "    \"\"\"\n",
    "    Ensures that the input value is converted to a datetime object.\n",
    "\n",
    "    If the input is already a datetime object, it is returned as-is. \n",
    "    If the input is a date object, it is combined with a time of 00:00:00 to create a datetime object.\n",
    "    If the input is neither a datetime nor a date object, the original value is returned.\n",
    "\n",
    "    Args:\n",
    "        value: The input value to be converted, which may be of type datetime, date, or other types.\n",
    "\n",
    "    Returns:\n",
    "        datetime: The converted datetime object or the original value if it's not convertible.\n",
    "    \"\"\"\n",
    "    if isinstance(value, datetime):\n",
    "        return value\n",
    "    elif hasattr(value, 'year') and hasattr(value, 'month') and hasattr(value, 'day'):\n",
    "        return datetime.combine(value, dt_time.min)\n",
    "    return value\n",
    "\n",
    "# Function to detect average speed violations between camera pairs\n",
    "def detect_average_speed_violations():\n",
    "    \"\"\"\n",
    "    Analyzes vehicle movements across multiple camera locations to detect average speed violations.\n",
    "    Input: \n",
    "        Camera event buffer with vehicle timestamps\n",
    "    Algorithm:\n",
    "        1. Group events by car_plate\n",
    "        2. For each vehicle with multiple events:\n",
    "           a. Sort events by timestamp\n",
    "           b. For consecutive events at different cameras:\n",
    "              i. Calculate distance between cameras\n",
    "              ii. Calculate time difference\n",
    "              iii. Compute average speed = distance / time\n",
    "              iv. If average speed > min(speed_limit1, speed_limit2) + margin:\n",
    "                 - Create violation record\n",
    "                 - Mark events as processed\n",
    "        3. Return violations list\n",
    "    \n",
    "    Returns:\n",
    "        list: Collection of violation records for vehicles exceeding average speed limits\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get all camera positions for distance calculation\n",
    "        camera_positions = {\n",
    "            camera[\"camera_id\"]: {\n",
    "                \"position\": camera[\"position\"],\n",
    "                \"speed_limit\": camera[\"speed_limit\"]\n",
    "            }\n",
    "            for camera in camera_collection.find({}, {\"_id\": 0, \"camera_id\": 1, \"position\": 1, \"speed_limit\": 1})\n",
    "        }\n",
    "\n",
    "        # Group camera events by car_plate\n",
    "        pipeline = [\n",
    "            {\"$group\": {\n",
    "                \"_id\": \"$car_plate\",\n",
    "                \"events\": {\"$push\": {\n",
    "                    \"event_id\": \"$event_id\",\n",
    "                    \"camera_id\": \"$camera_id\",\n",
    "                    \"timestamp\": \"$timestamp\",\n",
    "                    \"speed_reading\": \"$speed_reading\",\n",
    "                    \"processed\": {\"$ifNull\": [\"$processed\", False]}\n",
    "                }},\n",
    "                \"count\": {\"$sum\": 1}\n",
    "            }},\n",
    "            {\"$match\": {\"count\": {\"$gte\": 2}}},\n",
    "            {\"$limit\": 1000}\n",
    "        ]\n",
    "\n",
    "        vehicles_with_multiple_events = list(camera_event_buffer_collection.aggregate(pipeline))\n",
    "        violations = []\n",
    "\n",
    "        for vehicle in vehicles_with_multiple_events:\n",
    "            car_plate = vehicle[\"_id\"]\n",
    "            events = sorted(vehicle[\"events\"], key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "            for i in range(len(events) - 1):\n",
    "                if events[i][\"processed\"] or events[i+1][\"processed\"]:\n",
    "                    continue\n",
    "\n",
    "                start_event, end_event = events[i], events[i+1]\n",
    "                start_camera_id, end_camera_id = start_event[\"camera_id\"], end_event[\"camera_id\"]\n",
    "\n",
    "                if start_camera_id == end_camera_id:\n",
    "                    continue\n",
    "                if start_camera_id not in camera_positions or end_camera_id not in camera_positions:\n",
    "                    continue\n",
    "\n",
    "                distance_km = math.fabs(camera_positions[end_camera_id][\"position\"] - camera_positions[start_camera_id][\"position\"])\n",
    "                if distance_km < 0.1:\n",
    "                    continue\n",
    "\n",
    "                start_time, end_time = start_event[\"timestamp\"], end_event[\"timestamp\"]\n",
    "                time_diff_seconds = (end_time - start_time).total_seconds()\n",
    "                time_diff_hours = time_diff_seconds / 3600\n",
    "\n",
    "                if time_diff_seconds < 10 or time_diff_hours > 1:\n",
    "                    continue\n",
    "\n",
    "                avg_speed = distance_km / time_diff_hours\n",
    "                speed_limit = builtins.min(camera_positions[start_camera_id][\"speed_limit\"],\n",
    "                                           camera_positions[end_camera_id][\"speed_limit\"])\n",
    "\n",
    "                if avg_speed > speed_limit + SPEED_VIOLATION_MARGIN:\n",
    "                    violation = {\n",
    "                        \"violation_id\": str(uuid.uuid4()),\n",
    "                        \"car_plate\": car_plate,\n",
    "                        \"camera_id_start\": start_camera_id,\n",
    "                        \"camera_id_end\": end_camera_id,\n",
    "                        \"timestamp_start\": start_time,\n",
    "                        \"timestamp_end\": end_time,\n",
    "                        \"speed_reading\": float(avg_speed),\n",
    "                        \"speed_limit\": float(speed_limit),\n",
    "                        \"distance_km\": float(distance_km),\n",
    "                        \"time_diff_seconds\": int(time_diff_seconds),\n",
    "                        \"violation_type\": \"average_speed\",\n",
    "                        \"violation_date\": ensure_datetime(start_time.date()),\n",
    "                        \"processed\": False,\n",
    "                        \"notification_sent\": False,\n",
    "                        \"created_at\": datetime.now()\n",
    "                    }\n",
    "\n",
    "                    violations.append(violation)\n",
    "\n",
    "                    camera_event_buffer_collection.update_one(\n",
    "                        {\"event_id\": start_event[\"event_id\"]},\n",
    "                        {\"$set\": {\"processed\": True}}\n",
    "                    )\n",
    "                    camera_event_buffer_collection.update_one(\n",
    "                        {\"event_id\": end_event[\"event_id\"]},\n",
    "                        {\"$set\": {\"processed\": True}}\n",
    "                    )\n",
    "\n",
    "        return violations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting average speed violations: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b87043",
   "metadata": {},
   "source": [
    "## Save the violation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937e8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save violations to MongoDB\n",
    "def save_violations_to_mongodb(violations):\n",
    "    if not violations:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        bulk_operations = []\n",
    "\n",
    "        for violation in violations:\n",
    "            violation[\"violation_date\"] = ensure_datetime(violation[\"violation_date\"])\n",
    "\n",
    "            filter_doc = {\n",
    "                \"car_plate\": violation[\"car_plate\"],\n",
    "                \"violation_date\": violation[\"violation_date\"]\n",
    "            }\n",
    "\n",
    "            update_doc = {\n",
    "                \"$setOnInsert\": {\n",
    "                    \"first_violation_time\": violation[\"timestamp_start\"]\n",
    "                },\n",
    "                \"$set\": {\n",
    "                    \"last_updated\": datetime.now(),\n",
    "                    \"notification_sent\": False\n",
    "                },\n",
    "                \"$addToSet\": {\n",
    "                    \"violations\": violation\n",
    "                },\n",
    "                \"$inc\": {\n",
    "                    \"violation_count\": 1\n",
    "                }\n",
    "            }\n",
    "\n",
    "            bulk_operations.append(\n",
    "                UpdateOne(filter_doc, update_doc, upsert=True)\n",
    "            )\n",
    "\n",
    "        if bulk_operations:\n",
    "            result = violation_collection.bulk_write(bulk_operations)\n",
    "            return result.modified_count + result.upserted_count\n",
    "\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving violations to MongoDB: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a27d8e",
   "metadata": {},
   "source": [
    "# Logging Unmatched Events function\n",
    "\n",
    "The `log_unmatched_events` function is responsible for identifying and logging camera events that have not been matched within the expected timeframe. It processes events in chunks to efficiently manage the backlog of unmatched events.\n",
    "\n",
    "### Key Operations:\n",
    "1. **Event Querying**: \n",
    "   - Retrieves camera events from the `camera_event_buffer_collection` that have not been processed and have not yet been marked as unmatched.\n",
    "   \n",
    "2. **Metadata Addition**:\n",
    "   - Adds metadata to each event, including the reason for being unmatched (\"timeout\") and the age of the event.\n",
    "\n",
    "3. **Event Insertion**:\n",
    "   - Inserts the processed unmatched events into the `unmatched_event_collection` for logging purposes.\n",
    "\n",
    "4. **Event Update**:\n",
    "   - Marks the processed events as \"unmatched\" in the `camera_event_buffer_collection` to prevent them from being processed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7e6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_unmatched_events():\n",
    "    \"\"\"\n",
    "    Identifies and logs camera events that haven't been matched within the expected timeframe.\n",
    "\n",
    "    The function processes events in chunks to efficiently clear the backlog. It queries for events\n",
    "    that have not been processed and were not previously marked as unmatched. The function ensures that \n",
    "    events which have been unmatched for a certain period (30 minutes in this case) are logged and updated.\n",
    "\n",
    "    Operations performed:\n",
    "    - Retrieves unmatched events from the camera event buffer collection.\n",
    "    - Adds metadata to each event, including a reason for being unmatched and the age of the event.\n",
    "    - Inserts unmatched events into a logging collection (`unmatched_event_collection`).\n",
    "    - Marks the processed events as \"unmatched\" in the camera event buffer collection.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of unmatched events processed and logged.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_time = datetime.now()\n",
    "        safety_margin_minutes = 30\n",
    "        safety_cutoff = current_time - timedelta(minutes=safety_margin_minutes)\n",
    "        \n",
    "        # Process multiple chunks to clear backlog more efficiently\n",
    "        total_processed = 0\n",
    "        max_chunks = 5  # Process up to 5 chunks per batch run\n",
    "        \n",
    "        for chunk in range(max_chunks):\n",
    "            # Query for unmatched events\n",
    "            query = {\n",
    "                \"timestamp\": {\"$lt\": safety_cutoff},\n",
    "                \"processed\": False,\n",
    "                \"unmatched\": {\"$ne\": True}\n",
    "            }\n",
    "            \n",
    "            unmatched_events = list(camera_event_buffer_collection.find(query).limit(MAX_UNMATCHED_BATCH))\n",
    "            \n",
    "            if not unmatched_events:\n",
    "                # No more events to process\n",
    "                break\n",
    "            \n",
    "            # Prepare documents for the unmatched events collection\n",
    "            unmatched_docs = []\n",
    "            event_ids_to_update = []\n",
    "            \n",
    "            for event in unmatched_events:\n",
    "                unmatched_doc = event.copy()\n",
    "                unmatched_doc[\"_id\"] = str(uuid.uuid4())\n",
    "                unmatched_doc[\"unmatched_reason\"] = \"timeout\"\n",
    "                unmatched_doc[\"detection_age_seconds\"] = (current_time - event[\"timestamp\"]).total_seconds()\n",
    "                unmatched_doc[\"logged_at\"] = current_time\n",
    "                unmatched_doc[\"created_at\"] = current_time\n",
    "                \n",
    "                unmatched_docs.append(unmatched_doc)\n",
    "                event_ids_to_update.append(event[\"event_id\"])\n",
    "            \n",
    "            # Insert the unmatched events into the logging collection\n",
    "            if unmatched_docs:\n",
    "                unmatched_event_collection.insert_many(unmatched_docs)\n",
    "                \n",
    "                # Mark these events as unmatched\n",
    "                camera_event_buffer_collection.update_many(\n",
    "                    {\"event_id\": {\"$in\": event_ids_to_update}},\n",
    "                    {\"$set\": {\"unmatched\": True}}\n",
    "                )\n",
    "                \n",
    "                total_processed += len(unmatched_docs)\n",
    "            \n",
    "            # If this chunk was smaller than the limit, we've processed all available events\n",
    "            if len(unmatched_events) < MAX_UNMATCHED_BATCH:\n",
    "                break\n",
    "                \n",
    "        return total_processed\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging unmatched events: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af08503",
   "metadata": {},
   "source": [
    "# Process the batch\n",
    "\n",
    "The `process_batch` function processes a batch of camera events, detecting violations, logging unmatched events, and printing relevant statistics. It performs the following key tasks:\n",
    "\n",
    "1. **Save Events to Buffer**: Saves all events in the batch to a buffer collection for later processing.\n",
    "2. **Detect Violations**: Detects both average speed violations and instantaneous speed violations from the batch.\n",
    "3. **Save Violations to MongoDB**: Combines the detected violations and saves them to the MongoDB violations collection.\n",
    "4. **Log Unmatched Events**: Every 5th batch logs unmatched events, prints camera details, and violation statistics for monitoring purposes.\n",
    "5. **Processing Time Measurement**: Measures and logs the time taken to process the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33db48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Processes a batch of camera events, detecting and saving violations, logging unmatched events, \n",
    "    and printing camera details for monitoring. It also measures the processing time for each batch.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. **Save Events to Buffer**: All events in the batch are saved to a buffer for later processing.\n",
    "    2. **Detect Average Speed Violations**: Identifies violations based on average speed between camera pairs.\n",
    "    3. **Detect Instantaneous Speed Violations**: Identifies violations where vehicles exceed the speed limit at a single camera.\n",
    "    4. **Save Violations to MongoDB**: All detected violations are combined and saved to the violation collection in MongoDB.\n",
    "    5. **Log Unmatched Events**: Every 5th batch logs unmatched camera events and prints statistics about cameras and violations.\n",
    "    6. **Print Camera Details**: Prints information about active cameras and violation statistics every 5th batch.\n",
    "    7. **Measure Processing Time**: Calculates the time taken to process the batch and logs the results.\n",
    "\n",
    "    Parameters:\n",
    "        batch_df (DataFrame): A Spark DataFrame containing camera event data.\n",
    "        batch_id (int): The unique identifier for the batch being processed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing statistics about the batch, including:\n",
    "            - `batch_id`: The ID of the processed batch\n",
    "            - `events_processed`: The number of events processed in the batch\n",
    "            - `violations_detected`: The total number of violations detected\n",
    "            - `violations_saved`: The number of violations saved to MongoDB\n",
    "            - `unmatched_logged`: The number of unmatched events logged\n",
    "            - `processing_time_ms`: The time taken to process the batch in milliseconds\n",
    "            - `timestamp`: The timestamp when the batch was processed\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    batch_count = batch_df.count()\n",
    "    unmatched_count = 0\n",
    "    \n",
    "    print(f\"\\n---\\nProcessing batch {batch_id} with {batch_count} events at {datetime.now()}\")\n",
    "    \n",
    "    if batch_count == 0:\n",
    "        print(f\"Batch {batch_id} is empty, skipping processing\")\n",
    "        return {\n",
    "            \"batch_id\": batch_id,\n",
    "            \"events_processed\": 0,\n",
    "            \"violations_detected\": 0,\n",
    "            \"violations_saved\": 0,\n",
    "            \"processing_time_ms\": 0,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "    \n",
    "    # 1. Save all events to buffer for later processing\n",
    "    save_to_event_buffer(batch_df, batch_id)\n",
    "    \n",
    "    # 2. Check for average speed violations (ONLY ONCE)\n",
    "    average_violations = detect_average_speed_violations()\n",
    "    print(f\"Detected {len(average_violations)} average speed violations\")\n",
    "    \n",
    "    # 3. Check for instantaneous speed violations\n",
    "    instantaneous_violations_df = detect_instantaneous_violations(batch_df, camera_metadata_df)\n",
    "    instantaneous_count = instantaneous_violations_df.count()\n",
    "    print(f\"Detected {instantaneous_count} instantaneous speed violations\")\n",
    "    \n",
    "    # Convert to Python objects for MongoDB insertion\n",
    "    instantaneous_violations = []\n",
    "    if instantaneous_count > 0:\n",
    "        # Convert to Pandas for easier processing\n",
    "        violations_pandas = instantaneous_violations_df.toPandas()\n",
    "        \n",
    "        for _, row in violations_pandas.iterrows():\n",
    "            violation = {\n",
    "                \"violation_id\": str(uuid.uuid4()),\n",
    "                \"car_plate\": row[\"car_plate\"],\n",
    "                \"camera_id_start\": row[\"camera_id\"],\n",
    "                \"camera_id_end\": row[\"camera_id\"],  # Same as start for instantaneous\n",
    "                \"timestamp_start\": pd.to_datetime(row[\"timestamp\"]),\n",
    "                \"timestamp_end\": pd.to_datetime(row[\"timestamp\"]),  # Same as start for instantaneous\n",
    "                \"speed_reading\": float(row[\"speed_reading\"]),\n",
    "                \"speed_limit\": float(row[\"speed_limit\"]),\n",
    "                \"violation_type\": row[\"violation_type\"],\n",
    "                \"violation_date\": pd.to_datetime(row[\"timestamp\"]).date(),\n",
    "                \"processed\": False,\n",
    "                \"notification_sent\": False,\n",
    "                \"created_at\": datetime.now()\n",
    "            }\n",
    "            instantaneous_violations.append(violation)\n",
    "    \n",
    "    # 4. Combine and save all violations\n",
    "    all_violations = instantaneous_violations + average_violations\n",
    "    saved_count = save_violations_to_mongodb(all_violations)\n",
    "    \n",
    "    # Skip unmatched logging for this batch\n",
    "    # 5. Log unmatched events and print camera details every 5 batches\n",
    "    if batch_id % 20 == 0:  \n",
    "        # Log unmatched events\n",
    "        unmatched_count = log_unmatched_events()\n",
    "        print(f\"Logged {unmatched_count} unmatched camera events\")\n",
    "        \n",
    "        # Print camera details\n",
    "        print(\"\\n=== Camera Details (Every 10 batches) ===\")\n",
    "        camera_data = list(camera_collection.find({}, {\"_id\": 0}))\n",
    "        print(f\"Total cameras in system: {len(camera_data)}\")\n",
    "        \n",
    "        # Print active cameras (those with recent events)\n",
    "        recent_events = camera_event_buffer_collection.aggregate([\n",
    "            {\"$match\": {\"timestamp\": {\"$gt\": datetime.now() - timedelta(minutes=30)}}},\n",
    "            {\"$group\": {\"_id\": \"$camera_id\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nActive cameras in last 30 minutes:\")\n",
    "        for camera in recent_events:\n",
    "            print(f\"Camera ID: {camera['_id']}, Events: {camera['count']}\")\n",
    "            \n",
    "        # Print violation statistics\n",
    "        violation_stats = violation_collection.aggregate([\n",
    "            {\"$unwind\": \"$violations\"},\n",
    "            {\"$group\": {\n",
    "                \"_id\": \"$violations.violation_type\", \n",
    "                \"count\": {\"$sum\": 1},\n",
    "                \"avg_speed\": {\"$avg\": \"$violations.speed_reading\"}\n",
    "            }}\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nViolation Statistics:\")\n",
    "        for stat in violation_stats:\n",
    "            print(f\"Type: {stat['_id']}, Count: {stat['count']}, Avg Speed: {stat['avg_speed']:.2f} km/h\")\n",
    "        \n",
    "        print(\"=============================\\n\")\n",
    "    \n",
    "    # Calculate processing time\n",
    "    processing_time_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    print(f\"Batch {batch_id}: Processed {batch_count} events in {processing_time_ms} ms\")\n",
    "    print(f\"Detected {len(all_violations)} violations, saved {saved_count} records to MongoDB\")\n",
    "    print(f\"---\\n\")\n",
    "    \n",
    "    # Return stats for monitoring\n",
    "    return {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"events_processed\": batch_count,\n",
    "        \"violations_detected\": len(all_violations),\n",
    "        \"violations_saved\": saved_count,\n",
    "        \"unmatched_logged\": unmatched_count,\n",
    "        \"processing_time_ms\": processing_time_ms,\n",
    "        \"timestamp\": datetime.now()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755a35e",
   "metadata": {},
   "source": [
    "# Parse Kafka Stream\n",
    "\n",
    "The `parse_kafka_stream` function processes a raw Kafka stream, extracting and parsing the message data into a structured format. It performs the following key tasks:\n",
    "\n",
    "1. **Extract Message Value**: Converts the Kafka message to a string.\n",
    "2. **Parse JSON Data**: Converts the raw JSON string into structured data using a predefined schema (`camera_event_schema`).\n",
    "3. **Add Metadata**:\n",
    "   - Adds a `received_at` timestamp when the event is received.\n",
    "   - Calculates an `expiry_time` set to 30 minutes after the `received_at` timestamp.\n",
    "4. **Convert Timestamp**: Converts the `timestamp` field from a string to a proper `timestamp` type.\n",
    "5. **Apply Watermark**: Uses a watermark on the `timestamp` column to handle late-arriving data, based on the predefined `WATERMARK_DELAY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc518fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query started!\n",
      "Processing camera events with batch interval of 10 seconds\n",
      "Using watermark delay of 5 minutes for late events\n",
      "Kafka bootstrap servers: 172.25.240.1:9092\n",
      "Kafka topics: camera_events\n",
      "\n",
      "---\n",
      "Processing batch 1417 with 9142 events at 2025-05-29 10:40:45.743421\n",
      "Saved 9142 events to buffer from batch 1417\n",
      "Detected 326 average speed violations\n",
      "Detected 4276 instantaneous speed violations\n",
      "Batch 1417: Processed 9142 events in 22611 ms\n",
      "Detected 4602 violations, saved 4602 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1418 with 95 events at 2025-05-29 10:41:07.150403\n",
      "Saved 95 events to buffer from batch 1418\n",
      "Detected 347 average speed violations\n",
      "Detected 44 instantaneous speed violations\n",
      "Batch 1418: Processed 95 events in 8773 ms\n",
      "Detected 391 violations, saved 391 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1419 with 45 events at 2025-05-29 10:41:16.301023\n",
      "Saved 45 events to buffer from batch 1419\n",
      "Detected 28 average speed violations\n",
      "Detected 21 instantaneous speed violations\n",
      "Batch 1419: Processed 45 events in 5129 ms\n",
      "Detected 49 violations, saved 49 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1420 with 25 events at 2025-05-29 10:41:21.696100\n",
      "Saved 25 events to buffer from batch 1420\n",
      "Detected 1 average speed violations\n",
      "Detected 12 instantaneous speed violations\n",
      "Logged 8157 unmatched camera events\n",
      "\n",
      "=== Camera Details (Every 10 batches) ===\n",
      "Total cameras in system: 3\n",
      "\n",
      "Active cameras in last 30 minutes:\n",
      "\n",
      "Violation Statistics:\n",
      "Type: instantaneous_speed, Count: 12449, Avg Speed: 136.96 km/h\n",
      "Type: average_speed, Count: 1481, Avg Speed: 132.88 km/h\n",
      "=============================\n",
      "\n",
      "Batch 1420: Processed 25 events in 4857 ms\n",
      "Detected 13 violations, saved 13 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1421 with 43 events at 2025-05-29 10:41:30.759212\n",
      "Saved 43 events to buffer from batch 1421\n",
      "Detected 0 average speed violations\n",
      "Detected 21 instantaneous speed violations\n",
      "Batch 1421: Processed 43 events in 4028 ms\n",
      "Detected 21 violations, saved 21 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1422 with 48 events at 2025-05-29 10:41:40.791710\n",
      "Saved 48 events to buffer from batch 1422\n",
      "Detected 2 average speed violations\n",
      "Detected 20 instantaneous speed violations\n",
      "Batch 1422: Processed 48 events in 4442 ms\n",
      "Detected 22 violations, saved 22 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1423 with 44 events at 2025-05-29 10:41:50.769476\n",
      "Saved 44 events to buffer from batch 1423\n",
      "Detected 0 average speed violations\n",
      "Detected 17 instantaneous speed violations\n",
      "Batch 1423: Processed 44 events in 3969 ms\n",
      "Detected 17 violations, saved 17 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1424 with 48 events at 2025-05-29 10:42:00.773916\n",
      "Saved 48 events to buffer from batch 1424\n",
      "Detected 4 average speed violations\n",
      "Detected 25 instantaneous speed violations\n",
      "Batch 1424: Processed 48 events in 3188 ms\n",
      "Detected 29 violations, saved 29 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1425 with 48 events at 2025-05-29 10:42:10.855365\n",
      "Saved 48 events to buffer from batch 1425\n",
      "Detected 19 average speed violations\n",
      "Detected 24 instantaneous speed violations\n",
      "Batch 1425: Processed 48 events in 4315 ms\n",
      "Detected 43 violations, saved 43 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1426 with 47 events at 2025-05-29 10:42:20.740482\n",
      "Saved 47 events to buffer from batch 1426\n",
      "Detected 2 average speed violations\n",
      "Detected 24 instantaneous speed violations\n",
      "Batch 1426: Processed 47 events in 3425 ms\n",
      "Detected 26 violations, saved 26 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1427 with 45 events at 2025-05-29 10:42:30.751781\n",
      "Saved 45 events to buffer from batch 1427\n",
      "Detected 7 average speed violations\n",
      "Detected 20 instantaneous speed violations\n",
      "Batch 1427: Processed 45 events in 4032 ms\n",
      "Detected 27 violations, saved 27 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1428 with 46 events at 2025-05-29 10:42:40.730669\n",
      "Saved 46 events to buffer from batch 1428\n",
      "Detected 1 average speed violations\n",
      "Detected 27 instantaneous speed violations\n",
      "Batch 1428: Processed 46 events in 3399 ms\n",
      "Detected 28 violations, saved 28 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1429 with 46 events at 2025-05-29 10:42:50.759590\n",
      "Saved 46 events to buffer from batch 1429\n",
      "Detected 0 average speed violations\n",
      "Detected 20 instantaneous speed violations\n",
      "Batch 1429: Processed 46 events in 3553 ms\n",
      "Detected 20 violations, saved 20 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1430 with 45 events at 2025-05-29 10:43:00.747071\n",
      "Saved 45 events to buffer from batch 1430\n",
      "Detected 2 average speed violations\n",
      "Detected 15 instantaneous speed violations\n",
      "Batch 1430: Processed 45 events in 3460 ms\n",
      "Detected 17 violations, saved 17 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1431 with 46 events at 2025-05-29 10:43:10.810970\n",
      "Saved 46 events to buffer from batch 1431\n",
      "Detected 6 average speed violations\n",
      "Detected 25 instantaneous speed violations\n",
      "Batch 1431: Processed 46 events in 3588 ms\n",
      "Detected 31 violations, saved 31 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1432 with 45 events at 2025-05-29 10:43:20.441143\n",
      "Saved 45 events to buffer from batch 1432\n",
      "Detected 3 average speed violations\n",
      "Detected 18 instantaneous speed violations\n",
      "Batch 1432: Processed 45 events in 3299 ms\n",
      "Detected 21 violations, saved 21 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1433 with 47 events at 2025-05-29 10:43:30.489960\n",
      "Saved 47 events to buffer from batch 1433\n",
      "Detected 2 average speed violations\n",
      "Detected 22 instantaneous speed violations\n",
      "Batch 1433: Processed 47 events in 3140 ms\n",
      "Detected 24 violations, saved 24 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1434 with 48 events at 2025-05-29 10:43:40.517220\n",
      "Saved 48 events to buffer from batch 1434\n",
      "Detected 2 average speed violations\n",
      "Detected 23 instantaneous speed violations\n",
      "Batch 1434: Processed 48 events in 3677 ms\n",
      "Detected 25 violations, saved 25 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1435 with 45 events at 2025-05-29 10:43:50.551963\n",
      "Saved 45 events to buffer from batch 1435\n",
      "Detected 0 average speed violations\n",
      "Detected 22 instantaneous speed violations\n",
      "Batch 1435: Processed 45 events in 3256 ms\n",
      "Detected 22 violations, saved 22 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1436 with 47 events at 2025-05-29 10:44:00.596671\n",
      "Saved 47 events to buffer from batch 1436\n",
      "Detected 2 average speed violations\n",
      "Detected 20 instantaneous speed violations\n",
      "Batch 1436: Processed 47 events in 3469 ms\n",
      "Detected 22 violations, saved 22 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1437 with 46 events at 2025-05-29 10:44:10.618858\n",
      "Saved 46 events to buffer from batch 1437\n",
      "Detected 0 average speed violations\n",
      "Detected 21 instantaneous speed violations\n",
      "Batch 1437: Processed 46 events in 3255 ms\n",
      "Detected 21 violations, saved 21 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1438 with 45 events at 2025-05-29 10:44:20.668331\n",
      "Saved 45 events to buffer from batch 1438\n",
      "Detected 0 average speed violations\n",
      "Detected 15 instantaneous speed violations\n",
      "Batch 1438: Processed 45 events in 3368 ms\n",
      "Detected 15 violations, saved 15 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1439 with 49 events at 2025-05-29 10:44:30.696053\n",
      "Saved 49 events to buffer from batch 1439\n",
      "Detected 4 average speed violations\n",
      "Detected 21 instantaneous speed violations\n",
      "Batch 1439: Processed 49 events in 3878 ms\n",
      "Detected 25 violations, saved 25 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1440 with 46 events at 2025-05-29 10:44:40.729295\n",
      "Saved 46 events to buffer from batch 1440\n",
      "Detected 2 average speed violations\n",
      "Detected 23 instantaneous speed violations\n",
      "Logged 886 unmatched camera events\n",
      "\n",
      "=== Camera Details (Every 10 batches) ===\n",
      "Total cameras in system: 3\n",
      "\n",
      "Active cameras in last 30 minutes:\n",
      "\n",
      "Violation Statistics:\n",
      "Type: average_speed, Count: 1539, Avg Speed: 132.81 km/h\n",
      "Type: instantaneous_speed, Count: 12872, Avg Speed: 136.96 km/h\n",
      "=============================\n",
      "\n",
      "Batch 1440: Processed 46 events in 4384 ms\n",
      "Detected 25 violations, saved 25 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1441 with 44 events at 2025-05-29 10:44:50.760556\n",
      "Saved 44 events to buffer from batch 1441\n",
      "Detected 4 average speed violations\n",
      "Detected 15 instantaneous speed violations\n",
      "Batch 1441: Processed 44 events in 3897 ms\n",
      "Detected 19 violations, saved 19 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1442 with 47 events at 2025-05-29 10:45:00.749934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 47 events to buffer from batch 1442\n",
      "Detected 5 average speed violations\n",
      "Detected 24 instantaneous speed violations\n",
      "Batch 1442: Processed 47 events in 3356 ms\n",
      "Detected 29 violations, saved 29 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1443 with 46 events at 2025-05-29 10:45:10.752704\n",
      "Saved 46 events to buffer from batch 1443\n",
      "Detected 1 average speed violations\n",
      "Detected 24 instantaneous speed violations\n",
      "Batch 1443: Processed 46 events in 3734 ms\n",
      "Detected 25 violations, saved 25 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1444 with 44 events at 2025-05-29 10:45:20.740228\n",
      "Saved 44 events to buffer from batch 1444\n",
      "Detected 0 average speed violations\n",
      "Detected 17 instantaneous speed violations\n",
      "Batch 1444: Processed 44 events in 3633 ms\n",
      "Detected 17 violations, saved 17 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1445 with 45 events at 2025-05-29 10:45:30.737801\n",
      "Saved 45 events to buffer from batch 1445\n",
      "Detected 0 average speed violations\n",
      "Detected 22 instantaneous speed violations\n",
      "Batch 1445: Processed 45 events in 3469 ms\n",
      "Detected 22 violations, saved 22 records to MongoDB\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "Processing batch 1446 with 44 events at 2025-05-29 10:45:40.715404\n",
      "Saved 44 events to buffer from batch 1446\n",
      "Detected 0 average speed violations\n",
      "Detected 21 instantaneous speed violations\n",
      "Batch 1446: Processed 44 events in 3526 ms\n",
      "Detected 21 violations, saved 21 records to MongoDB\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_kafka_stream(kafka_df):\n",
    "    \"\"\"\n",
    "    Parses the incoming Kafka stream, extracting the message value and converting it to a structured format.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. **Extracts the Kafka message value**: The raw Kafka message is cast to a string.\n",
    "    2. **Parses the message value**: The raw JSON message is parsed into a structured format using the predefined `camera_event_schema`.\n",
    "    3. **Adds metadata fields**: The function adds two additional columns:\n",
    "        - `received_at`: Timestamp of when the event was received.\n",
    "        - `expiry_time`: Sets the expiry time of the event, 30 minutes after `received_at`.\n",
    "    4. **Converts timestamp**: The `timestamp` field in the parsed data is converted to a timestamp type.\n",
    "    5. **Watermark for late data**: The function applies a watermark to handle late-arriving data by setting the `timestamp` column as the watermark.\n",
    "\n",
    "    Parameters:\n",
    "        kafka_df (DataFrame): A streaming DataFrame representing raw Kafka messages.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A transformed streaming DataFrame with parsed values, metadata columns, and watermarks.\n",
    "    \"\"\"\n",
    "    # Extract the message value as string and convert to structured data using our schema\n",
    "    events_df = (\n",
    "        kafka_df\n",
    "        .selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "        .withColumn(\"parsed_value\", from_json(col(\"json_value\"), camera_event_schema))\n",
    "        .select(\"parsed_value.*\")\n",
    "        .withColumn(\"received_at\", current_timestamp())\n",
    "        .withColumn(\"expiry_time\", expr(\"received_at + interval 30 minutes\"))  \n",
    "    )\n",
    "    \n",
    "    # Convert string timestamp to timestamp type and apply watermark for late data\n",
    "    return events_df.withColumn(\n",
    "        \"timestamp\", \n",
    "        to_timestamp(col(\"timestamp\"))\n",
    "    ).withWatermark(\"timestamp\", WATERMARK_DELAY)  # Add watermark for late data\n",
    "\n",
    "# Define the streaming query with better error handling\n",
    "try:\n",
    "    # Create the Kafka stream\n",
    "    raw_kafka_stream = create_kafka_stream()\n",
    "    \n",
    "    # Parse the stream\n",
    "    camera_events_stream = parse_kafka_stream(raw_kafka_stream)\n",
    "    \n",
    "    # Start the streaming query with improved exception handling\n",
    "    streaming_query = camera_events_stream \\\n",
    "        .writeStream \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints/awas\") \\\n",
    "        .trigger(processingTime=BATCH_INTERVAL) \\\n",
    "        .start()\n",
    "    \n",
    "    print(\"Streaming query started!\")\n",
    "    print(f\"Processing camera events with batch interval of {BATCH_INTERVAL}\")\n",
    "    print(f\"Using watermark delay of {WATERMARK_DELAY} for late events\")\n",
    "    print(f\"Kafka bootstrap servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "    print(f\"Kafka topics: {', '.join(KAFKA_TOPICS)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error starting streaming query: {e}\")\n",
    "    print(traceback.format_exc())  # Print full stack trace for better debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef87edb",
   "metadata": {},
   "source": [
    "## 3. System Monitoring & Performance Analysis\n",
    "\n",
    "### 3.1 Streaming Metrics\n",
    "\n",
    "The `display_streaming_metrics()` function provides comprehensive visibility into the system's performance in real-time. We track several critical Key Performance Indicators (KPIs):\n",
    "\n",
    "| **Metric** | **Description** | **Ideal Range** | **Troubleshooting** |\n",
    "|------------|-----------------|-----------------|---------------------|\n",
    "| **Input Rate** | Number of events ingested per second | > 0, depends on traffic | Check Kafka connectivity if zero |\n",
    "| **Processing Rate** | Number of events processed per second | Close to input rate | Optimize batch processing if significantly lower |\n",
    "| **Batch Duration** | Time to process each micro-batch (ms) | < batch interval | Increase parallelism or batch interval if exceeded |\n",
    "| **Database Stats** | Event counts in buffer and violations | Growing appropriately | Check MongoDB connectivity if static |\n",
    "\n",
    "### 3.2 Watermarks & Late Data Handling\n",
    "\n",
    "Our system implements sophisticated handling of out-of-order and late-arriving data:\n",
    "\n",
    "```\n",
    "Event Time: ─────┬────────┬────────┬────────┬────────┬────────┬─────►\n",
    "                 │        │        │        │        │\n",
    "Watermark:  ─────┴────────┴────────┴────────┴────────┴────────┴─────►\n",
    "                          │                 │\n",
    "                          ▼                 │\n",
    "                   Within watermark         ▼\n",
    "                   (Processed normally)    Late event\n",
    "                                          (Special handling)\n",
    "```\n",
    "\n",
    "- **Watermark Configuration**: Set to `WATERMARK_DELAY = \"5 minutes\"` to accommodate reasonable network delays\n",
    "- **Late Event Strategy**: Events arriving after watermark are still processed but flagged for special handling\n",
    "- **Expiration Policy**: TTL index on `expiry_time` field automatically removes expired events\n",
    "\n",
    "### 3.3 Error Handling Strategy\n",
    "\n",
    "The system implements a multi-layered approach to error handling:\n",
    "\n",
    "1. **Function-Level Exception Handling**: Each function has try-except blocks with detailed error logging\n",
    "2. **Query-Level Monitoring**: The streaming query status is continuously monitored \n",
    "3. **Data Quality Checks**: Events with missing or invalid fields are logged rather than causing failures\n",
    "4. **Recovery Mechanism**: Using checkpointing to allow restart after failures without data loss\n",
    "5. **Unmatched Events**: Systematic logging of events that don't find a match within expected timeframes\n",
    "\n",
    "This comprehensive approach ensures system resilience and provides troubleshooting capabilities for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e603c76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is active! Monitoring for 300 seconds...\n",
      ".....Database stats: 0 events in buffer, 5383 violations recorded\n",
      ".....Database stats: 0 events in buffer, 5383 violations recorded\n",
      ".....Database stats: 9142 events in buffer, 5383 violations recorded\n",
      ".....Database stats: 9142 events in buffer, 5383 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:41:07.072725:\n",
      "  Input rate: 0.00 rows/sec\n",
      "  Processing rate: 1860.32 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 9142 events in buffer, 5383 violations recorded\n",
      ".....Database stats: 9237 events in buffer, 5383 violations recorded\n",
      "....\n",
      "Query Progress at 2025-05-29 10:41:16.158620:\n",
      "  Input rate: 19.31 rows/sec\n",
      "  Processing rate: 51.74 rows/sec\n",
      "  Batch duration: 0 ms\n",
      ".Database stats: 9237 events in buffer, 5384 violations recorded\n",
      "....\n",
      "Query Progress at 2025-05-29 10:41:21.210445:\n",
      "  Input rate: 24.50 rows/sec\n",
      "  Processing rate: 40.74 rows/sec\n",
      "  Batch duration: 0 ms\n",
      ".Database stats: 9282 events in buffer, 5384 violations recorded\n",
      "....\n",
      "Query Progress at 2025-05-29 10:41:26.254210:\n",
      "  Input rate: 22.63 rows/sec\n",
      "  Processing rate: 24.24 rows/sec\n",
      "  Batch duration: 0 ms\n",
      ".Database stats: 9307 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9350 events in buffer, 5384 violations recorded\n",
      "..\n",
      "Query Progress at 2025-05-29 10:41:34.380897:\n",
      "  Input rate: 23.44 rows/sec\n",
      "  Processing rate: 49.75 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "...Database stats: 9350 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9398 events in buffer, 5384 violations recorded\n",
      "...\n",
      "Query Progress at 2025-05-29 10:41:45.528886:\n",
      "  Input rate: 24.00 rows/sec\n",
      "  Processing rate: 50.84 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "..Database stats: 9398 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9442 events in buffer, 5384 violations recorded\n",
      "..\n",
      "Query Progress at 2025-05-29 10:41:54.619450:\n",
      "  Input rate: 22.00 rows/sec\n",
      "  Processing rate: 52.08 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "...Database stats: 9442 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9490 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:42:03.695711:\n",
      "  Input rate: 24.00 rows/sec\n",
      "  Processing rate: 69.50 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9490 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9538 events in buffer, 5384 violations recorded\n",
      "..\n",
      "Query Progress at 2025-05-29 10:42:14.825791:\n",
      "  Input rate: 24.00 rows/sec\n",
      "  Processing rate: 51.98 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "...Database stats: 9538 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9585 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:42:23.913720:\n",
      "  Input rate: 23.50 rows/sec\n",
      "  Processing rate: 64.23 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9585 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9630 events in buffer, 5384 violations recorded\n",
      "..\n",
      "Query Progress at 2025-05-29 10:42:35.011247:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 52.57 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "...Database stats: 9630 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9676 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:42:44.135845:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 63.17 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9676 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9722 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:42:54.221995:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 60.75 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9722 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9767 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:43:04.327492:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 60.86 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9767 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9813 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:43:14.419161:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 59.55 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9813 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9858 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:43:24.537264:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 63.38 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9858 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:43:33.609054:\n",
      "  Input rate: 23.50 rows/sec\n",
      "  Processing rate: 69.26 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 9905 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9905 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9953 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:43:44.762613:\n",
      "  Input rate: 24.00 rows/sec\n",
      "  Processing rate: 61.48 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 9953 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:43:53.823065:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 64.53 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 9998 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 9998 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:44:03.912025:\n",
      "  Input rate: 23.50 rows/sec\n",
      "  Processing rate: 62.20 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10045 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10045 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:44:14.032200:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 66.19 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10091 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10091 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:44:24.132555:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 62.15 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10136 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10136 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:44:34.236454:\n",
      "  Input rate: 24.50 rows/sec\n",
      "  Processing rate: 59.34 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10185 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10185 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10231 events in buffer, 5384 violations recorded\n",
      ".\n",
      "Query Progress at 2025-05-29 10:44:45.361988:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 49.93 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "....Database stats: 10231 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:44:54.442197:\n",
      "  Input rate: 22.00 rows/sec\n",
      "  Processing rate: 52.86 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10275 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10275 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:45:04.561240:\n",
      "  Input rate: 23.50 rows/sec\n",
      "  Processing rate: 65.10 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10322 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10322 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:45:14.678687:\n",
      "  Input rate: 23.00 rows/sec\n",
      "  Processing rate: 57.67 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10368 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10368 events in buffer, 5384 violations recorded\n",
      ".....\n",
      "Query Progress at 2025-05-29 10:45:24.777239:\n",
      "  Input rate: 22.00 rows/sec\n",
      "  Processing rate: 57.07 rows/sec\n",
      "  Batch duration: 0 ms\n",
      "Database stats: 10412 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10412 events in buffer, 5384 violations recorded\n",
      "....\n",
      "Query Progress at 2025-05-29 10:45:33.870390:\n",
      "  Input rate: 22.50 rows/sec\n",
      "  Processing rate: 60.79 rows/sec\n",
      "  Batch duration: 0 ms\n",
      ".Database stats: 10457 events in buffer, 5384 violations recorded\n",
      ".....Database stats: 10457 events in buffer, 5384 violations recorded\n",
      "....\n",
      "Query Progress at 2025-05-29 10:45:43.951101:\n",
      "  Input rate: 22.00 rows/sec\n",
      "  Processing rate: 57.94 rows/sec\n",
      "  Batch duration: 0 ms\n",
      ".\n",
      "Streaming metrics display complete\n",
      "Query is still active and running\n",
      "Metrics display stopped, but streaming query is still active\n",
      "Query status: Active\n"
     ]
    }
   ],
   "source": [
    "def display_streaming_metrics(query, seconds=10):\n",
    "    \"\"\"\n",
    "    Displays streaming query metrics at regular intervals with improved diagnostics.\n",
    "\n",
    "    This function monitors the status of the provided streaming query and prints relevant metrics, including:\n",
    "    - Input rate (rows per second)\n",
    "    - Processing rate (rows per second)\n",
    "    - Batch duration (in milliseconds)\n",
    "    \n",
    "    It also checks if the query is still active, handles possible errors, and provides diagnostics for common issues.\n",
    "    If the query is not active, it attempts to display the last error and its cause.\n",
    "\n",
    "    Parameters:\n",
    "        query: The streaming query to monitor.\n",
    "        seconds (int, optional): The number of seconds to monitor the query. Defaults to 10 seconds.\n",
    "    \n",
    "    Returns:\n",
    "        None: This function prints the query status and metrics to the console.\n",
    "    \"\"\"\n",
    "    last_progress = None\n",
    "    \n",
    "    # First check if the query is active\n",
    "    if not query or not query.isActive:\n",
    "        print(\"ERROR: Query is not active!\")\n",
    "        \n",
    "        # Try to get the last error\n",
    "        if query:\n",
    "            exception = query.exception()\n",
    "            if exception:\n",
    "                print(f\"Query exception: {exception}\")\n",
    "                print(f\"Root cause: {exception.__cause__}\")\n",
    "            \n",
    "            # Check query status\n",
    "            print(f\"Query status details: {query.status}\")\n",
    "            print(f\"Query last progress: {query.lastProgress}\")\n",
    "        \n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"1. Kafka server not available at\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "        print(\"2. Topic doesn't exist:\", ', '.join(KAFKA_TOPICS))\n",
    "        print(\"3. Error in process_batch function\")\n",
    "        print(\"\\nTry:\")\n",
    "        print(\"- Check your Kafka connection\")\n",
    "        print(\"- Verify MongoDB connection\")\n",
    "        print(\"- Look for exceptions in the output above\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    print(f\"Query is active! Monitoring for {seconds} seconds...\")\n",
    "    \n",
    "    for i in range(seconds):\n",
    "        if not query.isActive:\n",
    "            print(\"\\nWARNING: Query stopped running during monitoring!\")\n",
    "            exception = query.exception()\n",
    "            if exception:\n",
    "                print(f\"Query exception: {exception}\")\n",
    "            break\n",
    "            \n",
    "        # Get the latest query progress\n",
    "        progress = query.lastProgress\n",
    "        \n",
    "        if progress != last_progress and progress is not None:\n",
    "            print(f\"\\nQuery Progress at {datetime.now()}:\")\n",
    "            print(f\"  Input rate: {progress.get('inputRowsPerSecond', 0):.2f} rows/sec\")\n",
    "            print(f\"  Processing rate: {progress.get('processedRowsPerSecond', 0):.2f} rows/sec\")\n",
    "            print(f\"  Batch duration: {progress.get('batchDuration', 0)} ms\")\n",
    "            last_progress = progress\n",
    "        \n",
    "        # Check if any data is being received from Kafka\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            try:\n",
    "                # Count events in the buffer to see if data is flowing\n",
    "                event_count = camera_event_buffer_collection.count_documents({})\n",
    "                violation_count = violation_collection.count_documents({})\n",
    "                print(f\"Database stats: {event_count} events in buffer, {violation_count} violations recorded\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking database stats: {e}\")\n",
    "        \n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"\\nStreaming metrics display complete\")\n",
    "    \n",
    "    # Final check\n",
    "    if query.isActive:\n",
    "        print(\"Query is still active and running\")\n",
    "    else:\n",
    "        print(\"WARNING: Query is no longer active!\")\n",
    "        if query.exception():\n",
    "            print(f\"Query failed with: {query.exception()}\")\n",
    "# Keep the cell running for a while to process streams\n",
    "# You can stop it manually when needed\n",
    "try:\n",
    "    # Process for 5 minutes or until interrupted\n",
    "    display_streaming_metrics(streaming_query, seconds=300)  # 5 minutes\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStreaming interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during streaming: {e}\")\n",
    "finally:\n",
    "    # Don't stop the streaming query automatically\n",
    "    # so it can continue running for further cells\n",
    "    print(\"Metrics display stopped, but streaming query is still active\")\n",
    "    print(f\"Query status: {'Active' if streaming_query.isActive else 'Stopped'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30802cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping streaming query...\n",
      "Streaming query stopped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_streaming_query(query):\n",
    "    \"\"\"\n",
    "    Gracefully stops an active streaming query.\n",
    "\n",
    "    This function checks if the provided query is active. If the query is active, \n",
    "    it stops the query and prints a message indicating that the query has been stopped.\n",
    "    If the query is not active, it prints a message indicating that there is no active query to stop.\n",
    "\n",
    "    Parameters:\n",
    "        query: The streaming query to be stopped.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns `True` if the query was stopped successfully, `False` if there was no active query to stop.\n",
    "    \"\"\"\n",
    "    if query and query.isActive:\n",
    "        print(f\"Stopping streaming query...\")\n",
    "        query.stop()\n",
    "        print(f\"Streaming query stopped\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No active streaming query to stop\")\n",
    "        return False\n",
    "stop_streaming_query(streaming_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1c3e9",
   "metadata": {},
   "source": [
    "## 4. Conclusion & System Assessment\n",
    "\n",
    "### 4.1 System Benefits\n",
    "\n",
    "The Automated Traffic Violation Detection System (AWAS) provides several significant benefits:\n",
    "\n",
    "1. **Real-time Processing**: The system detects violations as they occur, enabling immediate response.\n",
    "2. **Dual Detection Methods**: By implementing both instantaneous and average speed detection, the system can identify different types of traffic violations.\n",
    "3. **Fault Tolerance**: The use of checkpointing, exception handling, and unmatched event logging makes the system resilient to failures.\n",
    "4. **Scalability**: The architecture supports horizontal scaling by adding more Kafka partitions and Spark workers.\n",
    "5. **Data Integrity**: The system uses MongoDB's transaction support and bulk operations to ensure data consistency.\n",
    "\n",
    "### 4.2 System Limitations\n",
    "\n",
    "Despite its capabilities, the system has several limitations that could be addressed in future work:\n",
    "\n",
    "1. **Limited Vehicle Information**: The system relies only on license plate recognition and doesn't incorporate vehicle type or color.\n",
    "2. **Fixed Speed Thresholds**: Speed violation thresholds are static and don't adjust for weather or traffic conditions.\n",
    "3. **No Image Verification**: Unlike real-world systems, our implementation doesn't store photographic evidence of violations.\n",
    "4. **Single-Source Truth**: The system assumes camera readings are accurate without cross-validation from multiple sensors.\n",
    "5. **Batch Window Constraints**: The current implementation might miss correlations that span beyond the configured time windows.\n",
    "\n",
    "### 4.3 Future Enhancements\n",
    "\n",
    "The system could be enhanced in several ways:\n",
    "\n",
    "1. **Machine Learning Integration**: Add anomaly detection to identify suspicious patterns or equipment malfunctions.\n",
    "2. **Geographic Partitioning**: Shard data based on geographic zones for better scalability.\n",
    "3. **Dynamic Thresholds**: Implement adaptive thresholds based on time of day, weather conditions, and traffic density.\n",
    "4. **Notification System**: Add a real-time notification system for immediate alerting of severe violations.\n",
    "5. **Dashboard Integration**: Develop a real-time dashboard for traffic monitoring and system health visualization.\n",
    "\n",
    "### 4.4 References\n",
    "\n",
    "1. Apache Spark. (2021). *Structured Streaming Programming Guide*. https://spark.apache.org/docs/3.1.1/structured-streaming-programming-guide.html\n",
    "\n",
    "2. MongoDB, Inc. (2021). *MongoDB Documentation*. https://docs.mongodb.com/v5.0/\n",
    "\n",
    "3. Apache Kafka. (2021). *Apache Kafka Documentation*. https://kafka.apache.org/28/documentation.html\n",
    "\n",
    "4. Armbrust, M., Das, T., Torres, J., Yavuz, B., Zhu, S., Xin, R., ... & Zaharia, M. (2018). *Structured streaming: A declarative API for real-time applications in Apache Spark*. In Proceedings of the 2018 International Conference on Management of Data (pp. 601-613).\n",
    "\n",
    "5. Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust, M., Dave, A., ... & Stoica, I. (2016). *Apache Spark: A unified engine for big data processing*. Communications of the ACM, 59(11), 56-65.\n",
    "\n",
    "6. Kleppmann, M. (2017). *Designing data-intensive applications: The big ideas behind reliable, scalable, and maintainable systems*. O'Reilly Media, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
